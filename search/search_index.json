{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Noodle is a lightweight convolutional neural network inference library designed for microcontroller units with very limited RAM.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>API Reference Documentation</li> <li>MLPerf Tiny Anomaly Detection on ESP32 </li> <li>MLPerf Tiny Visual Wake Word on ESP32 </li> <li>Simple 1D Peak Detection</li> <li>Single Digit Recognition with Touch-Based Interface<ul> <li>LeNet-5 implementation on ESP32</li> <li>Two layer fully connected network on ATmega328 (Arduino Uno R3)</li> <li>LeNet-5 on Raspberry Pico</li> <li>SD-Card\u2013Backed LeNet-5 Inference</li> <li>Mobile-LeNet</li> </ul> </li> </ul>"},{"location":"#persistent-identifier","title":"Persistent identifier","text":""},{"location":"#authors","title":"Authors","text":"<ul> <li>Auralius Manurung \u2014 Universitas Telkom, Bandung (auralius.manurung@ieee.org)</li> <li>Lisa Kristiana \u2014 ITENAS, Bandung (lisa@itenas.ac.id)</li> </ul>"},{"location":"ad-esp32/","title":"Anomaly Detection on ESP32","text":""},{"location":"ad-esp32/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>Download the whole project files here (Visual Code and PlatformIO).</li> <li>Google Colab link. This is used to extract stored weights and biases from <code>ad01_fp32.tflite</code>.</li> <li>Weights, biases and test datasets. These are extracted from the provided <code>ad01_fp32.tflite</code>.</li> </ul> </li> </ul>"},{"location":"ad-esp32/#mlperf-tiny-anomaly-detection-model","title":"MLPerf Tiny anomaly detection model","text":"<p>In this section, we will implement the anomali detection which is one of benchmark found in MLPerf Tiny. The network is entirely made of dense layers with ReLU activations (Auto-encoder Fully-Connected or AEFC). </p> Layer # Type Input dim Output dim Activation Extracted weights and biases 1 Dense 640 128 ReLU <code>w01.txt</code>, <code>w02.txt</code> 2 Dense 128 128 ReLU <code>w03.txt</code>, <code>w04.txt</code> 3 Dense 128 128 ReLU <code>w05.txt</code>, <code>w06.txt</code> 4 Dense 128 128 ReLU <code>w07.txt</code>, <code>w08.txt</code> 5 Dense 128 8 ReLU <code>w09.txt</code>, <code>w10.txt</code> 6 Dense 8 128 ReLU <code>w11.txt</code>, <code>w12.txt</code> 7 Dense 128 128 ReLU <code>w13.txt</code>, <code>w14.txt</code> 8 Dense 128 128 ReLU <code>w15.txt</code>, <code>w16.txt</code> 9 Dense 128 128 ReLU <code>w17.txt</code>, <code>w18.txt</code> 10 Dense 128 640 None (Linear) <code>w19.txt</code>, <code>w20.txt</code>"},{"location":"ad-esp32/#extracting-the-weights-and-biases","title":"Extracting the weights and biases","text":"<p>The repository provides weights and biases that we can use directly. For this purpose, we selected the trained network stored in <code>ad01_fp32.tflite</code>.</p>"},{"location":"ad-esp32/#generating-txt-from-wav-files","title":"Generating <code>.txt</code> from <code>.wav</code> files","text":"<p>For our benchmark test, we perform the same preprocessing pipeline used during training (as found in this part of the repository). However, we will run the test offline and with viewer data. The ESP32 never processes raw audio. It only consumes precomputed feature vectors stored as text files. Each input <code>.wav</code> file is converted into multiple fixed-length feature vectors using the following steps:</p> <ul> <li>Load WAV file: the audio file is loaded using its native sampling rate (no resampling). The WAV file is 11 seconds with 342 frames.</li> <li>Feature extraction: a log-mel spectrogram is computed using the same parameters defined in <code>baseline.yaml</code>:<ul> <li><code>n_mels = 128</code></li> <li><code>frames = 5</code></li> <li><code>n_fft = 1024</code></li> <li><code>hop_length = 512</code></li> <li><code>power = 2.0</code></li> </ul> </li> <li>Temporal cropping: only the central portion of the spectrogram is kept: <code>frames 50 to 250 \u2192 200</code> frames total.</li> <li>Sliding window segmentation: a sliding window of length frames = 5 is applied across the cropped spectrogram, producing: <code>200 \u2212 5 + 1 = 196</code> feature vectors per WAV file.</li> <li>Flattening and storage: each window is flattened into a 1-D vector of size: <code>inputDim = n_mels \u00d7 frames = 128 \u00d7 5 = 640</code> and stored as a <code>float32</code> text file:</li> </ul> <pre><code>&lt;wav_name&gt;_part000.txt\n&lt;wav_name&gt;_part001.txt\n\u2026\n&lt;wav_name&gt;_part195.txt\n</code></pre> <ul> <li>Take 5 parts from the an anomaly set and name them <code>anom1.txt</code> to <code>anom5.txt</code>. </li> <li>Take 5 parts from the a normal set and name them <code>norm1.txt</code> to <code>norm5.txt</code>. </li> </ul> <p>These <code>.txt</code> files represent the actual inputs to the auto-encoder and match exactly the data format used during training and evaluation in the original baseline implementation.</p>"},{"location":"ad-esp32/#esp32-inference-workflow","title":"ESP32 inference workflow","text":"<p>On the ESP32, we will perform the following inference procedures:</p> <ul> <li>For a given WAV sample, all corresponding <code>*_partXXX.bin</code> files are loaded sequentially from SD card (or FFAT).</li> <li>Each .bin file is read into a <code>float[640]</code> input buffer.</li> <li>The input vector is passed through the auto-encoder implemented using Noodle, producing a reconstructed output vector of the same size.</li> <li>The mean squared reconstruction error (MSE) between input and output is computed for that window.</li> <li>Errors are accumulated across all 196 windows.</li> <li>The final anomaly score for the WAV file is computed as the average reconstruction error: <code>score = mean(MSE_part_0 \u2026 MSE_part_195)</code></li> </ul> <p>We will only retain the final scalar score and discard the individually reconstructed vectors immediately to minimize memory usage.</p>"},{"location":"ad-esp32/#hardware","title":"Hardware","text":"<p>For this benchmark, we will use ESP32-S3-N16R8 which gives us plenty of space in the flash.</p>"},{"location":"ad-esp32/#testing-scenario","title":"Testing scenario","text":"<ul> <li>Apply 5 parts from an anomaly dataset \u279c 5/196 of a full WAV.</li> <li>Apply 5 parts from a normal dataset \u279c 5/196 of a full WAV.</li> <li>Each part is <code>float32[640]</code> values (2560 bytes).</li> <li>ESP32 returns <code>mse</code> and elapsed time (<code>us</code>) for each part.</li> </ul>"},{"location":"ad-esp32/#code-on-the-esp-side","title":"Code on the ESP side","text":"<pre><code>static constexpr uint16_t INPUT_DIM = 640;\nstatic constexpr uint16_t HIDDEN_DIM = 128;\nstatic constexpr uint16_t BOTTLENECK_DIM = 8;\n\n// Ping-pong buffers\nstatic float BUF1[INPUT_DIM];\nstatic float BUF2[INPUT_DIM];\n\n// Copy of input for MSE\nstatic float X0[INPUT_DIM];\n\nFCNFile L1;  \nL1.weight_fn  = \"/w01.txt\"; L1.bias_fn  = \"/w02.txt\"; L1.act  = ACT_RELU;\n\nFCNFile L2;  \nL2.weight_fn  = \"/w03.txt\"; L2.bias_fn  = \"/w04.txt\"; L2.act  = ACT_RELU;\n\nFCNFile L3;  \nL3.weight_fn  = \"/w05.txt\"; L3.bias_fn  = \"/w06.txt\"; L3.act  = ACT_RELU;\n\nFCNFile L4;  \nL4.weight_fn  = \"/w07.txt\"; L4.bias_fn  = \"/w08.txt\"; L4.act  = ACT_RELU;\n\nFCNFile L5;  \nL5.weight_fn  = \"/w09.txt\"; L5.bias_fn  = \"/w10.txt\"; L5.act  = ACT_RELU;\n\nFCNFile L6;  \nL6.weight_fn  = \"/w11.txt\"; L6.bias_fn  = \"/w12.txt\"; L6.act  = ACT_RELU;\n\nFCNFile L7;  \nL7.weight_fn  = \"/w13.txt\"; L7.bias_fn  = \"/w14.txt\"; L7.act  = ACT_RELU;\n\nFCNFile L8;  \nL8.weight_fn  = \"/w15.txt\"; L8.bias_fn  = \"/w16.txt\"; L8.act  = ACT_RELU;\n\nFCNFile L9;  \nL9.weight_fn  = \"/w17.txt\"; L9.bias_fn  = \"/w18.txt\"; L9.act  = ACT_RELU;\n\nFCNFile L10; \nL10.weight_fn = \"/w19.txt\"; L10.bias_fn = \"/w20.txt\"; L10.act = ACT_NONE;\n\nuint16_t V = INPUT_DIM;\n\n// 640 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 8 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 640\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L1,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L2,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L3,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L4,  NULL);\nV = noodle_fcn(BUF1, V, BOTTLENECK_DIM,  BUF2, L5,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L6,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L7,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L8,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L9,  NULL);\nV = noodle_fcn(BUF2, V, INPUT_DIM,       BUF1, L10, NULL);\n</code></pre>"},{"location":"ad-esp32/#compare-esp32-and-google-colab","title":"Compare ESP32 and Google Colab","text":"<p>ESP 32</p> <pre><code>=== anom set ===\nanom1: mse=9.17803478 us=23812318\nanom2: mse=8.465662 us=23812164\nanom3: mse=8.66184902 us=23812145\nanom4: mse=9.07823086 us=23812145\nanom5: mse=8.99418163 us=23812144\nMean anom MSE = 8.87559128\n\n=== norm set ===\nnorm1: mse=10.8290482 us=23812112\nnorm2: mse=11.2695885 us=23812183\nnorm3: mse=11.530509 us=23812167\nnorm4: mse=11.6987247 us=23812152\nnorm5: mse=11.0928583 us=23812148\nMean norm MSE = 11.2841454\n\nDONE (processed anom1..5 + norm1..5)\n</code></pre> <p>Google Colab</p> <ul> <li>Google Colab link</li> <li>Weights, biases and test datasets</li> </ul> <pre><code>Input : input_1 shape= [  1 640] dtype= &lt;class 'numpy.float32'&gt;\nOutput: Identity shape= [  1 640] dtype= &lt;class 'numpy.float32'&gt;\n\n--- ANOM ---\n[TFLite] /content/sample_data/anom1.txt: mse=9.17908482\n[TFLite] /content/sample_data/anom2.txt: mse=8.46575602\n[TFLite] /content/sample_data/anom3.txt: mse=8.6624254\n[TFLite] /content/sample_data/anom4.txt: mse=9.07872793\n[TFLite] /content/sample_data/anom5.txt: mse=8.99621678\n\n--- NORM ---\n[TFLite] /content/sample_data/norm1.txt: mse=10.82921\n[TFLite] /content/sample_data/norm2.txt: mse=11.2699071\n[TFLite] /content/sample_data/norm3.txt: mse=11.5304345\n[TFLite] /content/sample_data/norm4.txt: mse=11.6980121\n[TFLite] /content/sample_data/norm5.txt: mse=11.0927933\n\n--- SUMMARY ---\nnorm mean=11.2840714 std=0.344865398 min=10.82921 max=11.6980121\nanom mean=8.87644219 std=0.300551306 min=8.46575602 max=9.17908482\nanom/norm mean ratio = 0.786634706\n\nDone.\n</code></pre>"},{"location":"ad-esp32/#inference-parity-esp32-vs-colab","title":"Inference parity: ESP32 vs Colab","text":""},{"location":"ad-esp32/#anomaly-set","title":"Anomaly set","text":"Sample ESP32 MSE TFLite MSE \u0394 <code>anom1.txt</code> 9.1780 9.1791 ~0.001 <code>anom2.txt</code> 8.4657 8.4658 ~0.0001 <code>anom3.txt</code> 8.6618 8.6624 ~0.0006 <code>anom4.txt</code> 9.0782 9.0787 ~0.0005 <code>anom5.txt</code> 8.9942 8.9962 ~0.002"},{"location":"ad-esp32/#normal-set","title":"Normal set","text":"Sample ESP32 MSE TFLite MSE \u0394 (abs) <code>norm1.txt</code> 10.82897 10.82921 \u2248 0.00024 <code>norm2.txt</code> 11.27035 11.26991 \u2248 0.00044 <code>norm3.txt</code> 11.53070 11.53043 \u2248 0.00027 <code>norm4.txt</code> 11.69870 11.69801 \u2248 0.00069 <code>norm5.txt</code> 11.09396 11.09279 \u2248 0.00117"},{"location":"api/","title":"Brief API References","text":"<p>Tiny, file-streamed CNN/ML primitives for microcontrollers with very small RAM budgets.  </p> <p>Noodle supports in-memory and file-system-streamed (SD/FFat/SD_MMC/LittleFS/SdFat) execution for:</p> <ul> <li>2D/1D convolution (+ optional pooling)</li> <li>bias + activation</li> <li>flatten</li> <li>fully connected (FCN)</li> <li>softmax / sigmoid</li> </ul> <p>This reference is generated from <code>noodle.h</code>, <code>noodle_config.h</code>, and <code>noodle_fs.h</code>.</p>"},{"location":"api/#1-configuration","title":"1) Configuration","text":""},{"location":"api/#11-filesystem-backend-selection-noodle_configh-noodle_fsh","title":"1.1 Filesystem backend selection (<code>noodle_config.h</code> / <code>noodle_fs.h</code>)","text":"<p>Define exactly one before including <code>noodle.h</code>:</p> <ul> <li><code>NOODLE_USE_SDFAT</code></li> <li><code>NOODLE_USE_SD_MMC</code></li> <li><code>NOODLE_USE_FFAT</code></li> <li><code>NOODLE_USE_LITTLEFS</code></li> </ul> <p><code>noodle_fs.h</code> exposes:</p> <ul> <li><code>using NDL_File = ...</code> </li> <li><code>FsFile</code> for SdFat, otherwise Arduino <code>File</code></li> <li><code>NOODLE_FS</code> handle  </li> <li><code>SdFat NOODLE_FS</code> (extern, defined in <code>noodle.cpp</code>) or the FS singleton (<code>FFat</code>, <code>SD_MMC</code>, <code>LittleFS</code>)</li> <li>Backend-neutral open/remove helpers:</li> <li><code>NDL_File noodle_fs_open_read(const char* path)</code></li> <li><code>NDL_File noodle_fs_open_write(const char* path)</code></li> <li><code>bool noodle_fs_remove(const char* path)</code></li> </ul>"},{"location":"api/#12-pooling-mode-noodle_configh","title":"1.2 Pooling mode (<code>noodle_config.h</code>)","text":"<p>Compile-time pooling mode for 2D pooling:</p> <ul> <li><code>#define NOODLE_POOL_MAX  1</code></li> <li><code>#define NOODLE_POOL_MEAN 2</code></li> <li><code>#define NOODLE_POOL_MODE NOODLE_POOL_MAX</code> or <code>NOODLE_POOL_MEAN</code></li> </ul>"},{"location":"api/#2-naming-convention-for-streamed-channel-files-two-letter-indices","title":"2) Naming convention for streamed channel files (two-letter indices)","text":""},{"location":"api/#convolution-kernels-4d-tensors","title":"Convolution kernels (4D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;&lt;in&gt;&lt;out&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> <li><code>&lt;in&gt;</code> is a two-letter code for the input-channel index (<code>aa</code>, <code>ab</code>, \u2026, <code>zz</code>)</li> <li><code>&lt;out&gt;</code> is a two-letter code for the output-channel index</li> </ul> <p>Example: <code>w01aaab.txt</code></p> <p>Each file contains one flattened spatial kernel for a single <code>(input channel, output channel)</code> pair.</p>"},{"location":"api/#dense-weights-2d-tensors","title":"Dense weights (2D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> </ul> <p>Example: <code>w03.txt</code></p> <p>Dense weights are exported as a transposed, flattened array, one value per line.</p>"},{"location":"api/#bias-vectors-1d-tensors","title":"Bias vectors (1D tensors)","text":"<p>Filename format: <code>b&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a bias vector</li> </ul> <p>Example: <code>b02.txt</code></p> <p>Each file contains flattened bias values, one value per line.</p>"},{"location":"api/#3-types","title":"3) Types","text":""},{"location":"api/#31-activation","title":"3.1 Activation","text":"<pre><code>enum Activation : uint8_t { ACT_NONE = 0, ACT_RELU = 1, ACT_SOFTMAX = 2 };\n</code></pre>"},{"location":"api/#32-conv-shared-params-for-1d2d","title":"3.2 <code>Conv</code> (shared params for 1D/2D)","text":"<pre><code>struct Conv {\n  uint16_t K;              // kernel size (2D: KxK, 1D: K)\n  uint16_t P = 0;          // padding\n  uint16_t S = 1;          // stride\n  const char *weight_fn;   // file template for weights\n  const char *bias_fn;     // bias filename\n  Activation act = ACT_RELU;\n};\n</code></pre>"},{"location":"api/#33-pool","title":"3.3 <code>Pool</code>","text":"<pre><code>struct Pool {\n  uint16_t M = 2;  // pool kernel\n  uint16_t T = 2;  // pool stride\n};\n</code></pre>"},{"location":"api/#34-progress-callback","title":"3.4 Progress callback","text":"<pre><code>typedef void (*CBFPtr)(float progress);  // progress in [0,1]\n</code></pre>"},{"location":"api/#35-fcn-parameter-blocks","title":"3.5 FCN parameter blocks","text":"<pre><code>struct FCN     \n{ \n  const char *weight_fn; \n  const char *bias_fn; \n  Activation act = ACT_RELU; \n};\n\nstruct FCNFile \n{ \n  const char *weight_fn; \n  const char *bias_fn; \n  Activation act = ACT_RELU; \n};\n\nstruct FCNMem {\n  const float *weight;  // row-major [n_outputs, n_inputs]\n  const float *bias;    // length n_outputs\n  Activation act = ACT_RELU;\n};\n</code></pre>"},{"location":"api/#4-initialization-scratch-buffers","title":"4) Initialization &amp; scratch buffers","text":""},{"location":"api/#41-temporary-buffers-required-for-some-streamed-ops","title":"4.1 Temporary buffers (required for some streamed ops)","text":"<pre><code>void noodle_setup_temp_buffers(void *b1, void *b2 = NULL);\n</code></pre> <p>Provides two reusable scratch buffers used internally by file-streaming convolution/FCN variants.</p> <p>Typical guidance (from header docs): - temp buffer #1: \u2265 <code>W*W*sizeof(input_element)</code> - temp buffer #2: \u2265 <code>W*W*sizeof(float)</code> (accumulator)</p>"},{"location":"api/#5-filesystem-utilities","title":"5) Filesystem utilities","text":""},{"location":"api/#51-open-for-write-delete-first","title":"5.1 Open for write (delete first)","text":"<pre><code>NDL_File noodle_open_file_for_write(const char* fn);\n</code></pre>"},{"location":"api/#52-read-bytes-until-terminator","title":"5.2 Read bytes until terminator","text":"<pre><code>size_t noodle_read_bytes_until(NDL_File &amp;file, char terminator, char *buffer, size_t length);\n</code></pre> <p>Reads into <code>buffer</code> (always NUL-terminated), stops at <code>terminator</code> or <code>length-1</code>.</p>"},{"location":"api/#53-backend-init","title":"5.3 Backend init","text":"<pre><code>bool noodle_sd_init(int clk_pin, int cmd_pin, int d0_pin);\nbool noodle_sd_init();\n</code></pre> <p>Pins variant is meaningful for SD_MMC; default form uses backend defaults.</p>"},{"location":"api/#54-delete-file","title":"5.4 Delete file","text":"<pre><code>void noodle_delete_file(const char *fn);\n</code></pre>"},{"location":"api/#6-scalar-io-helpers-text-files","title":"6) Scalar I/O helpers (text files)","text":"<pre><code>void  noodle_write_float(NDL_File &amp;f, float d);\nfloat noodle_read_float(NDL_File &amp;f);\n\nbyte  noodle_read_byte(NDL_File &amp;f);\nvoid  noodle_write_byte(NDL_File &amp;f, byte d);\n</code></pre>"},{"location":"api/#7-memory-helpers","title":"7) Memory helpers","text":"<pre><code>float* noodle_create_buffer(uint16_t size); \nvoid   noodle_delete_buffer(float *buffer);\nvoid   noodle_reset_buffer(float *buffer, uint16_t n); \n</code></pre>"},{"location":"api/#8-array-grid-io-text-files","title":"8) Array / grid I/O (text files)","text":""},{"location":"api/#81-write","title":"8.1 Write","text":"<pre><code>void noodle_array_to_file(float *array, const char *fn, uint16_t n);\nvoid noodle_grid_to_file(byte  *grid,  const char *fn, uint16_t n); /\nvoid noodle_grid_to_file(float *grid,  const char *fn, uint16_t n);\n</code></pre>"},{"location":"api/#82-read","title":"8.2 Read","text":"<pre><code>void noodle_array_from_file(const char *fn, float *buffer, uint16_t K);\n\nvoid noodle_grid_from_file(const char *fn, byte   *buffer, uint16_t K);\nvoid noodle_grid_from_file(const char *fn, int8_t *buffer, uint16_t K);\nvoid noodle_grid_from_file(const char *fn, float  *buffer, uint16_t K);\n</code></pre>"},{"location":"api/#83-padded-accessors","title":"8.3 Padded accessors","text":"<pre><code>float noodle_get_padded_x(byte  *grid, int16_t i, int16_t j, int16_t W, int16_t P);\nfloat noodle_get_padded_x(float *grid, int16_t i, int16_t j, int16_t W, int16_t P);\n</code></pre> <p>Returns 0 outside bounds, otherwise grid value as float.</p>"},{"location":"api/#9-convolution-pooling-primitives","title":"9) Convolution &amp; pooling primitives","text":""},{"location":"api/#91-2d-convolution-in-memory-primitive","title":"9.1 2D convolution (in-memory primitive)","text":"<p>Output spatial size: <code>V = (W - K + 2P)/S + 1</code></p> <pre><code>uint16_t noodle_do_conv(byte  *grid, float *kernel, uint16_t K, uint16_t W,\n                        float *output, uint16_t P, uint16_t S);\n\nuint16_t noodle_do_conv(float *grid, float *kernel, uint16_t K, uint16_t W,\n                        float *output, uint16_t P, uint16_t S);\n</code></pre>"},{"location":"api/#92-bias-optional-activation","title":"9.2 Bias (+ optional activation)","text":"<pre><code>uint16_t noodle_do_bias(float *output, float bias, uint16_t n);  // legacy: bias + ReLU\n\nuint16_t noodle_do_bias_act(float *output, float bias, uint16_t n, Activation act);\n</code></pre>"},{"location":"api/#93-2d-pooling-mode-selected-by-noodle_pool_mode","title":"9.3 2D pooling (mode selected by <code>NOODLE_POOL_MODE</code>)","text":"<p>Output spatial size: <code>V_out = (V - K)/S + 1</code></p> <pre><code>uint16_t noodle_do_pooling(float *input, uint16_t W, uint16_t K, uint16_t S, const char *fn);\nuint16_t noodle_do_pooling(const float *input, uint16_t W, uint16_t K, uint16_t S, float *output);\n</code></pre>"},{"location":"api/#94-1d-pooling-file-output","title":"9.4 1D pooling (file output)","text":"<pre><code>uint16_t noodle_do_pooling1d(float *input, uint16_t W, uint16_t K, uint16_t S, const char *fn);\n</code></pre>"},{"location":"api/#95-1d-convolution-primitive","title":"9.5 1D convolution primitive","text":"<p>Output length: <code>V = (W - K + 2P)/S + 1</code></p> <pre><code>uint16_t noodle_do_conv1d(float *input, float *kernel, uint16_t W, uint16_t K,\n                          float *output, uint16_t P, uint16_t S);\n</code></pre>"},{"location":"api/#10-streamed-convolution-apis","title":"10) Streamed Convolution APIs","text":"<p>These functions use the filename tokenization convention (two-letter indices) where applicable.</p>"},{"location":"api/#101-2d-convolution-optional-pooling","title":"10.1 2D convolution (+ optional pooling)","text":"<p>File \u2192 File (BYTE input feature maps)</p> <pre><code>uint16_t noodle_conv_byte(const char *in_fn,\n                          uint16_t n_inputs,\n                          uint16_t n_outputs,\n                          const char *out_fn,\n                          uint16_t W,\n                          const Conv &amp;conv,\n                          const Pool &amp;pool,\n                          CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 File (FLOAT input feature maps)</p> <pre><code>uint16_t noodle_conv_float(const char *in_fn,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           const char *out_fn,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 Memory (FLOAT inputs, output tensor <code>[O, Wo, Wo]</code>)</p> <pre><code>uint16_t noodle_conv_float(const char *in_fn,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           float *output,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Memory \u2192 File (FLOAT inputs)</p> <pre><code>uint16_t noodle_conv_float(float *input,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           const char *out_fn,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Memory \u2192 Memory (FLOAT inputs)</p> <pre><code>uint16_t noodle_conv_float(float *input,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           float *output,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Return value is the output spatial size after pooling (if enabled by your <code>Pool</code> settings).</p>"},{"location":"api/#102-1d-convolution-streamed-tokenized-filenames","title":"10.2 1D convolution (streamed, tokenized filenames)","text":"<p>File \u2192 File, with pooling</p> <pre><code>uint16_t noodle_conv1d(float *input, float *output,\n                       uint16_t n_inputs, uint16_t n_outputs,\n                       const char *in_fn, const char *out_fn,\n                       uint16_t W, const Conv &amp;conv, const Pool &amp;pool,\n                       CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 File, no pooling</p> <pre><code>uint16_t noodle_conv1d(float *input, float *output,\n                       uint16_t n_inputs, uint16_t n_outputs,\n                       const char *in_fn, const char *out_fn,\n                       uint16_t W, const Conv &amp;conv,\n                       CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#11-fully-connected-fcn","title":"11) Fully Connected (FCN)","text":"<p>All FCN variants compute: - <code>y = W\u00b7x + b</code> - then apply activation (<code>ACT_NONE</code> or <code>ACT_RELU</code> in typical usage; <code>ACT_SOFTMAX</code> exists but softmax is usually applied separately)</p>"},{"location":"api/#111-memory-int8-file-params-from-files","title":"11.1 Memory (int8) \u2192 File (params from files)","text":"<pre><code>uint16_t noodle_fcn(const int8_t *input, uint16_t n_inputs, uint16_t n_outputs,\n                    const char *out_fn, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#112-file-float-text-file-params-from-files","title":"11.2 File (float text) \u2192 File (params from files)","text":"<pre><code>uint16_t noodle_fcn(const char *in_fn, uint16_t n_inputs, uint16_t n_outputs,\n                    const char *out_fn, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#113-memory-float-memory-in-memory-params","title":"11.3 Memory (float) \u2192 Memory (in-memory params)","text":"<pre><code>uint16_t noodle_fcn(const float *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNMem &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#114-memory-byte-memory-params-from-files","title":"11.4 Memory (byte) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const byte *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#115-memory-int8-memory-params-from-files","title":"11.5 Memory (int8) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const int8_t *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#116-file-float-text-memory-params-from-files","title":"11.6 File (float text) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const char *in_fn, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#117-memory-float-memory-params-from-files","title":"11.7 Memory (float) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const float *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb);\n</code></pre>"},{"location":"api/#12-flatten","title":"12) Flatten","text":""},{"location":"api/#121-file-memory-flatten","title":"12.1 File \u2192 Memory flatten","text":"<pre><code>uint16_t noodle_flat(const char *in_fn, float *output, uint16_t V, uint16_t n_filters);\n</code></pre> <p>Reads <code>n_filters</code> feature maps from files named by <code>in_fn</code> template and writes a vector of length <code>V*V*n_filters</code>.</p>"},{"location":"api/#122-memory-memory-flatten","title":"12.2 Memory \u2192 Memory flatten","text":"<pre><code>uint16_t noodle_flat(float *input, float *output, uint16_t V, uint16_t n_filters);\n</code></pre>"},{"location":"api/#13-activations","title":"13) Activations","text":"<pre><code>uint16_t noodle_soft_max(float *input_output, uint16_t n);\nuint16_t noodle_sigmoid(float *input_output, uint16_t n);\n</code></pre>"},{"location":"api/#14-misc-utilities","title":"14) Misc utilities","text":""},{"location":"api/#141-tensor-slicing-helper","title":"14.1 Tensor slicing helper","text":"<pre><code>static inline float* noodle_slice(float* flat, size_t W, size_t z);\n</code></pre> <p>Slices a stacked <code>[Z, W, W]</code> tensor stored as contiguous planes.</p>"},{"location":"api/#142-read-first-line-of-a-text-file","title":"14.2 Read first line of a text file","text":"<pre><code>void noodle_read_top_line(const char* fn, char *line, size_t maxlen);\n</code></pre>"},{"location":"api/#notes","title":"Notes","text":"<ul> <li>Text file format: Tensor/weight/bias I/O uses human-readable text, \u201cone float per line\u201d.</li> <li>Filename templates are mutated in-place: pass writable <code>char[]</code> buffers.</li> </ul>"},{"location":"ecg-peak-detect/","title":"Peak Detection on ESP32","text":""},{"location":"ecg-peak-detect/#peak-detection-with-a-1d-convolutional-network-peaknet1d","title":"Peak Detection with a 1D Convolutional Network (PeakNet1D)","text":"<p>In this experiment, we implement a simple but powerful peak detection system using a one-dimensional convolutional neural network (PeakNet1D). The goal is to detect peaks (such as R-peaks in ECG signals) directly from raw time-domain data, without hand-crafted feature extraction.</p> <p>Google Colab Notebook can be found here.</p>"},{"location":"ecg-peak-detect/#1-input-representation","title":"1. Input representation","text":"<p>Each input window contains 256 samples, corresponding to a short segment of the signal (one second at 256 Hz). We can think of this as a vector:</p> \\[x=[x_0,x_1,x_2,\u2026,x_{255}] \\] \\[\\bar{x}=\\frac{x-\\text{mean}(x)}{\\text{std}(x)}\\] <p>where \\(x\\)  and \\(\\bar{x}\\) are the ECG signal and the normalized ECG signal, respectively. At this stage:</p> <ul> <li>there is only one channel</li> <li>each sample represents signal amplitude at a specific time index</li> </ul> <p>The network processes this window and produces 256 output values, one for each time step. Each output value represents how likely that time index corresponds to a peak.</p>"},{"location":"ecg-peak-detect/#2-output-representation","title":"2. Output representation","text":"<p>Instead of producing a single classification result (\u201cpeak\u201d or \u201cnot peak\u201d), PeakNet1D produces a score at every time index. This is achieved by using only convolutional layers with stride = 1 and \u201csame\u201d padding.</p> \\[y=[y_0,y_1,y_2,\u2026,y_{255}] \\] <p>For example, \\(y_0\\) is the probability that \\(x_0\\) is a signal peak.</p>"},{"location":"ecg-peak-detect/#3-layer-by-layer-intuition","title":"3. Layer-by-layer intuition","text":"<p>The network is composed of six convolutional layers:</p> <pre><code>// PeakNet1D layers\nConvMem c1; c1.K=9; c1.P=4; c1.S=1; c1.weight=w01; c1.bias=b01; c1.act=ACT_RELU; // 1-&gt;8\nConvMem c2; c2.K=7; c2.P=3; c2.S=1; c2.weight=w02; c2.bias=b02; c2.act=ACT_RELU; // 8-&gt;16\nConvMem c3; c3.K=7; c3.P=3; c3.S=1; c3.weight=w03; c3.bias=b03; c3.act=ACT_RELU; // 16-&gt;16\nConvMem c4; c4.K=7; c4.P=3; c4.S=1; c4.weight=w04; c4.bias=b04; c4.act=ACT_RELU; // 16-&gt;16\nConvMem c5; c5.K=1; c5.P=0; c5.S=1; c5.weight=w05; c5.bias=b05; c5.act=ACT_RELU; // 16-&gt;16\nConvMem c6; c6.K=1; c6.P=0; c6.S=1; c6.weight=w06; c6.bias=b06; c6.act=ACT_NONE; // 16-&gt;1\n\nuint16_t V = L;\n\n// noodle_conv1d(in, n_inputs, out, n_outputs, W, conv)\nV = noodle_conv1d(BUFFER3, 1,  BUFFER4, 8,  V, c1, NULL);\nV = noodle_conv1d(BUFFER4, 8,  BUFFER3, 16, V, c2, NULL);\nV = noodle_conv1d(BUFFER3, 16, BUFFER4, 16, V, c3, NULL);\nV = noodle_conv1d(BUFFER4, 16, BUFFER3, 16, V, c4, NULL);\nV = noodle_conv1d(BUFFER3, 16, BUFFER4, 16, V, c5, NULL);\nV = noodle_conv1d(BUFFER4, 16, BUFFER3, 1,  V, c6, NULL);\n\n// Final sigmoid on 1 channel output\nnoodle_sigmoid(BUFFER3, V);\n</code></pre> <p>The layers use larger kernels (7\u20139 samples wide). The number of channels increases (1 \u2192 8 \u2192 16), allowing the network to represent multiple feature types simultaneously. The last layer reduces the channel dimension from 16 to 1. After the final convolution, a sigmoid function maps each value into \\([0,1]\\), producing a peak likelihood signal.</p> <p>In the implementation, two large buffers are used (<code>BUFFER3</code> and <code>BUFFER4</code>) with alternating roles (input and output). This is known as ping-pong buffering.</p> Stage Active buffer Channels \u00d7 length Floats Input <code>BUFFER3</code> 1 \u00d7 256 256 After c1 <code>BUFFER4</code> 8 \u00d7 256 2048 After c2 <code>BUFFER3</code> 16 \u00d7 256 4096 After c3 <code>BUFFER4</code> 16 \u00d7 256 4096 After c4 <code>BUFFER3</code> 16 \u00d7 256 4096 After c5 <code>BUFFER4</code> 16 \u00d7 256 4096 After c6 <code>BUFFER3</code> 1 \u00d7 256 256"},{"location":"ecg-peak-detect/#4-from-scores-to-peaks","title":"4. From scores to peaks","text":"<p>The output of the network is not yet a list of peaks. Instead, it is a smooth score curve, ranging from 0 to 1:</p> \\[y[t]\u2208[0,1],\\,t=0,1,\u2026255 \\] <p>Peak detection is completed using simple thresholding (post-processing step):</p> \\[\\text{peak}(t)=1 \\, \\text{if} \\, y[t]&gt;0.5 \\] <p>There is also additional logic to perform refractory period enforcement (to avoid double counting).</p>"},{"location":"lenet-5-esp32-sdio/","title":"SD-Card\u2013Backed LeNet-5","text":""},{"location":"lenet-5-esp32-sdio/#sd-cardbacked-lenet-5-inference","title":"SD-Card\u2013Backed LeNet-5 Inference","text":"<p>This section evaluates a LeNet-5\u2013style inference pipeline in which weights, biases, and intermediate activations are streamed to and from an SD card, rather than retained fully in RAM. The target platform is an ESP32-P4-Pico, interfacing with external storage via 1-bit SDIO.</p> Step Op / Layer Input (where) Output (where) Parameters (where) Notes 1 Conv1 + Pool <code>GRID</code> (variable) <code>out1.txt</code> (file) <code>w01.txt</code>, <code>b01.txt</code> (files) <code>noodle_conv_float(\"in1.txt\", 1, 6, \"out1.txt\", 28, cnn1, pool, ...)</code> 2 Conv2 + Pool <code>out1.txt</code> (file) <code>out2.txt</code> (file) <code>w02.txt</code>, <code>b02.txt</code> (files) <code>noodle_conv_float(\"out1.txt\", 6, 16, \"out2.txt\", V, cnn2, pool, ...)</code> 3 Flatten (CHW file \u2192 HWC-flat RAM) <code>out2.txt</code> (file) <code>BUFFER1</code> (variable) \u2014 <code>V = noodle_flat(\"out2.txt\", BUFFER1, V, 16)</code> 4 FC1 (RAM \u2192 file) <code>BUFFER1</code> (variable) <code>out3.txt</code> (file) <code>w03.txt</code>, <code>b03.txt</code> (files) <code>V = noodle_fcn(BUFFER1, V, 120, \"out3.txt\", fcn1, ...)</code> 5 FC2 (file \u2192 file) <code>out3.txt</code> (file) <code>out4.txt</code> (file) <code>w04.txt</code>, <code>b04.txt</code> (files) <code>V = noodle_fcn(\"out3.txt\", V, 84, \"out4.txt\", fcn2, ...)</code> 6 FC3 + Softmax (file \u2192 RAM) <code>out4.txt</code> (file) <code>BUFFER2</code> (variable) <code>w05.txt</code>, <code>b05.txt</code> (files) <code>V = noodle_fcn(\"out4.txt\", V, 10, BUFFER2, fcn3, ...)</code> <p></p>"},{"location":"lenet-5-esp32/","title":"SD Card","text":""},{"location":"lenet-5-esp32/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>GitHub (Visual Code and PlatformIO).</li> <li>Google Colab notebook </li> <li>Dataset link</li> </ul> </li> <li>Video demonstration :</li> </ul>"},{"location":"lenet-5-esp32/#plan","title":"Plan","text":"<p>We aim to implement a slightly modified LeNet-5 network that runs entirely on-device on an ESP32. To reflect a realistic inference scenario, user input will be captured via a touchscreen interface. This means the project involves not only the neural network itself, but also a preprocessing pipeline that converts raw touch input into a digitized 28\u00d728 representation suitable for inference.</p> <p>In short, the system consists of:</p> <ul> <li>on-device CNN inference</li> <li>touchscreen-based data acquisition</li> <li>preprocessing and normalization tailored to embedded constraints</li> </ul> <p>In this documentation, we skip the preprocessing pipeline and focus on the neural network implementation using the NOODLE framework.</p> <p>Implementation steps:</p> <ol> <li>Design the network and train it (e.g., in Google Colab).</li> <li>Export weights and biases.</li> <li>Decide where each parameter lives:<ul> <li>File storage: SD card (SPI/SDIO), or flash filesystem (FFat/LittleFS)</li> <li>In-memory storage: <code>const</code> in flash, or SRAM/PSRAM</li> </ul> </li> <li>Implement the network layer-by-layer using NOODLE</li> </ol>"},{"location":"lenet-5-esp32/#cheap-yellow-display-cyd","title":"Cheap Yellow Display (CYD)","text":"<p>The CYD is an ESP32-based development board with an integrated LCD display and touchscreen. The display measures 2.8 inches with a 320\u00d7240 pixel resolution. Due to its low cost and decent performance, the CYD has gained popularity in the ESP32 community, with several public repositories and example projects available, such as ESP32-Cheap-Yellow-Display.</p> <p></p> <p>The CYD uses SPI for both the display and the touch controller. These are typically wired with separate chip-select lines (and on some CYD variants, separate SPI buses), which helps when tracking continuous touch input for handwriting.</p> <p>As an alternative, we can also use displays where the TFT uses a parallel bus (or SPI) while the touch uses analog inputs, such as MCUFRIEND_kbv UNO shields.</p>"},{"location":"lenet-5-esp32/#original-architecture-1998","title":"Original architecture (1998)","text":"<p>The following figure and table describe the original LeNet-5 architecture.</p> <p></p> Layer Type Kernel / Stride Input shape Output shape Notes C1 Convolution 6\u00d75\u00d75 1\u00d732\u00d732 6\u00d728\u00d728 tanh S2 Subsampling 2\u00d72 / 2 6\u00d728\u00d728 6\u00d714\u00d714 average pooling (+ learnable scale/bias in classic LeNet) C3 Convolution 16\u00d75\u00d75 6\u00d714\u00d714 16\u00d710\u00d710 tanh, sparse connection map S4 Subsampling 2\u00d72 / 2 16\u00d710\u00d710 16\u00d75\u00d75 average pooling C5 Convolution 120\u00d75\u00d75 16\u00d75\u00d75 120\u00d71\u00d71 effectively fully connected F6 Fully connected \u2014 120 84 tanh Output Fully connected \u2014 84 10 Euclidean RBF"},{"location":"lenet-5-esp32/#slightly-modified-architecture","title":"Slightly modified architecture","text":"<p>Because the C5 (120\u00d75\u00d75) layer is equivalent to a dense layer from 400 \u2192 120, we implement it as flatten + FC:</p> Layer Type Kernel / Stride Input shape Output shape Notes C1 Convolution 6\u00d75\u00d75 1\u00d732\u00d732 6\u00d728\u00d728 ReLU S2 Subsampling 2\u00d72 / 2 6\u00d728\u00d728 6\u00d714\u00d714 average pooling C3 Convolution 16\u00d75\u00d75 6\u00d714\u00d714 16\u00d710\u00d710 ReLU S4 Subsampling 2\u00d72 / 2 16\u00d710\u00d710 16\u00d75\u00d75 average pooling F5 Flatten \u2014 16\u00d75\u00d75 400 16\u00d75\u00d75 = 400 F6 FC \u2014 400 120 ReLU F7 FC \u2014 120 84 ReLU Output FC \u2014 84 10 Softmax"},{"location":"lenet-5-esp32/#training-keras-pytorch","title":"Training (Keras / PyTorch)","text":"<p>Noodle does not provide training capability. We will perform training on a computer, extract the training results and deploy them to the ESP32. In this section, we use Google Colab (Keras and Python) for training.</p> <ul> <li>Dataset link</li> <li>Google Colab notebook </li> </ul>"},{"location":"lenet-5-esp32/#naming-convention-txt","title":"Naming convention (<code>.txt</code>)","text":"<p>The provided Keras training program come with an automatic exporter function that will export weights and biases with the following naming convention.</p>"},{"location":"lenet-5-esp32/#convolution-kernels-4d-tensors","title":"Convolution kernels (4D tensors)","text":"<p>Filename format <code>w&lt;NN&gt;.txt</code></p> <p>Where <code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor (layer index).</p> <p>Example <code>w01.txt</code> \u2014 convolution weights for layer 1.</p> <p>File contents Each file stores all convolution kernels for a single layer, serialized into a 1D sequence.</p>"},{"location":"lenet-5-esp32/#dense-weights-2d-tensors","title":"Dense weights (2D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> </ul> <p>Example: <code>w03.txt</code></p> <p>Dense weights are exported as a transposed, flattened array, one value per line.</p>"},{"location":"lenet-5-esp32/#bias-vectors-1d-tensors","title":"Bias vectors (1D tensors)","text":"<p>Filename format: <code>b&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a bias vector</li> </ul> <p>Example: <code>b02.txt</code></p> <p>Each file contains flattened bias values, one value per line.</p> <p>The exporter program will print the generated files. These are some examples for the LeNet-5. Besides generating <code>TXT</code> files, the exporter also generates the header files. Thus, we can use them as <code>const</code> arrays stored in flash (variable-level mnemonics).</p> <pre><code>/content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w01.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w01.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b01.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b01.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w02.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w02.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b02.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b02.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w03.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w03.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b03.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b03.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w04.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w04.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b04.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b04.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w05.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/w05.h /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b05.txt /content/drive/MyDrive/NOODLE/datasets/mnist/lenet-5/b05.h\n</code></pre>"},{"location":"lenet-5-esp32/#on-the-esp32-side","title":"On the ESP32 side","text":"<p>For fully connected layers, NOODLE provides two parameter structures: <code>FCNMem</code> and <code>FCNFile</code>.</p> <pre><code>/** FCN parameters (filenames; no tokenization). */\nstruct FCNFile {\n  const char *weight_fn = nullptr;\n  const char *bias_fn   = nullptr;\n  Activation act = ACT_RELU;\n};\n\n/** FCN parameters for in-memory weights/bias (row-major weights [n_outputs, n_inputs]). */\nstruct FCNMem {\n  const float *weight = nullptr;\n  const float *bias   = nullptr;\n  Activation act = ACT_RELU;\n};\n</code></pre> <p>If weights/biases are in memory, we use <code>FCNMem</code>. If they are stored as files, we use <code>FCNFile</code>.</p>"},{"location":"lenet-5-esp32/#parameter-placement","title":"Parameter placement","text":"Layer Files Location In-memory (<code>const</code>) File-based C1 <code>w01.txt</code>, <code>b01.txt</code> FFat \u2714 C3 <code>w02.txt</code>, <code>b02.txt</code> FFat \u2714 F6 <code>w03.txt</code>, <code>b03.txt</code> flash (<code>const</code>) \u2714 F7 <code>w04.txt</code>, <code>b04.txt</code> flash (<code>const</code>) \u2714 Output <code>w05.txt</code>, <code>b05.txt</code> flash (<code>const</code>) \u2714"},{"location":"lenet-5-esp32/#layer-by-layer-implementation","title":"Layer-by-layer implementation","text":"<p>Here, we define access to CNN parameters as files while access to FCN parameters as variables.</p> <pre><code>Conv cnn1;\ncnn1.K = 5;\ncnn1.P = 2;\ncnn1.S = 1; // same padding\ncnn1.weight_fn = \"/w01.txt\";\ncnn1.bias_fn   = \"/b01.txt\";\n\nConv cnn2;\ncnn2.K = 5;\ncnn2.P = 0;\ncnn2.S = 1; // valid padding\ncnn2.weight_fn = \"/w02.txt\";\ncnn2.bias_fn   = \"/b02.txt\";\n\nPool pool;\npool.M = 2;\npool.T = 2;\n\nFCNMem fcn1;\nfcn1.weight = w03;\nfcn1.bias   = b03;\nfcn1.act    = ACT_RELU;\n\nFCNMem fcn2;\nfcn2.weight = w04;\nfcn2.bias   = b04;\nfcn2.act    = ACT_RELU;\n\nFCNMem fcn3;\nfcn3.weight = w05;\nfcn3.bias   = b05;\nfcn3.act    = ACT_SOFTMAX;\n\nuint16_t V;\n\nV = noodle_conv_float(BUFFER1, 1, 6,  BUFFER3, 28, cnn1, pool, nullptr);\nV = noodle_conv_float(BUFFER3, 6, 16, BUFFER1, V,  cnn2, pool, nullptr);\n\nV = noodle_flat(BUFFER1, BUFFER3, V, 16);\n\nV = noodle_fcn(BUFFER3, V, 120, BUFFER1, fcn1, nullptr);\nV = noodle_fcn(BUFFER1, V, 84,  BUFFER3, fcn2, nullptr);\nV = noodle_fcn(BUFFER3, V, 10,  BUFFER1, fcn3, nullptr);\n</code></pre> <p>This implementation requires three buffers:</p> <ul> <li><code>BUFFER1</code>: input buffer</li> <li><code>BUFFER2</code>: temporary/scratch buffer (set via <code>noodle_setup_temp_buffers()</code>)</li> <li><code>BUFFER3</code>: output buffer</li> </ul> <p>For the modified LeNet-5 implementation:</p> <pre><code>Input image      :    BUFFER1\nConv + Pool #1   :    BUFFER1 \u2192 BUFFER3\nConv + Pool #2   :    BUFFER3 \u2192 BUFFER1\nFlatten          :    BUFFER1 \u2192 BUFFER3\nDense #1         :    BUFFER3 \u2192 BUFFER1\nDense #2         :    BUFFER1 \u2192 BUFFER3\nDense #3         :    BUFFER3 \u2192 BUFFER1\n</code></pre>"},{"location":"lenet-5-esp32/#visual-code-with-platformio","title":"Visual Code with PlatformIO","text":"<p>We use Visual Studio Code with PlatformIO instead of the Arduino IDE primarily because it provides better support for file-system images on ESP32 devices (when compared to Arduino IDE). All files intended for the ESP32 filesystem are placed in the project\u2019s <code>data/</code> directory.</p> <p></p> <p>The workflows are as follows.</p> <ul> <li>Build a file-system image from the contents of <code>data/</code> (all files in <code>data</code> directory to one image file:  <code>fatfs.bin</code>).</li> <li>Upload image file to the file-system partition in flash.</li> <li>Leave the application firmware unchanged</li> </ul>"},{"location":"lenet-5-esp32/#benchmarking","title":"Benchmarking","text":"<p>To automate benchmarking, we deploy the classification model to an ESP32 and utilize a Python-based test harness. The harness applies random rotations within a range of \\(\\pm20^{\\circ}\\) before streaming the payloads to the ESP32 via serial communication for inference. These random rotations were not applied during training.</p>"},{"location":"lenet-5-esp32/#cnn-as-variables-fcn-as-variables","title":"CNN as variables -- FCN as variables","text":"<p>To put the CNN parameters in SRAM, we use <code>ConvMem</code> structure.</p> <pre><code>void predict(){\n  ConvMem cnn1;\n  cnn1.K = 5;\n  cnn1.P = 2;\n  cnn1.S = 1; // same padding\n  cnn1.weight = w01;\n  cnn1.bias   = b01;\n\n  ConvMem cnn2;\n  cnn2.K = 5;\n  cnn2.P = 0;\n  cnn2.S = 1; // valid padding\n  cnn2.weight = w02;\n  cnn2.bias   = b02;\n\n  Pool pool;\n  pool.M = 2;\n  pool.T = 2;\n\n  FCNMem fcn_mem1;\n  fcn_mem1.weight = w03;\n  fcn_mem1.bias   = b03;\n  fcn_mem1.act    = ACT_RELU;\n\n  FCNMem fcn_mem2;\n  fcn_mem2.weight = w04;\n  fcn_mem2.bias   = b04;\n  fcn_mem2.act    = ACT_RELU;\n\n  FCNMem fcn_mem3;\n  fcn_mem3.weight = w05;\n  fcn_mem3.bias   = b05;\n  fcn_mem3.act    = ACT_SOFTMAX;\n\n  unsigned long st = micros();\n  uint16_t V;\n\n  V = noodle_conv_float(BUFFER1, 1, 6, BUFFER3, 28, cnn1, pool, NULL);\n  V = noodle_conv_float(BUFFER3, 6, 16, BUFFER1, V, cnn2, pool, NULL);\n\n  V = noodle_flat(BUFFER1, BUFFER3, V, 16);\n\n  V = noodle_fcn(BUFFER3, V, 120, BUFFER1, fcn_mem1, NULL);\n  V = noodle_fcn(BUFFER1, V, 84,  BUFFER3, fcn_mem2, NULL);\n  V = noodle_fcn(BUFFER3, V, 10,  BUFFER1, fcn_mem3, NULL);\n  \u22ee\n  \u22ee\n}\n</code></pre> <p></p>"},{"location":"lenet-5-esp32/#cnn-as-files-fcn-as-variables","title":"CNN as files -- FCN as variables","text":"<p>To put the CNN parameters in File, we use <code>Conv</code> structure.</p> <pre><code>void predict()\n{\n  Conv cnn1;\n  cnn1.K = 5;\n  cnn1.P = 2;\n  cnn1.S = 1; // same padding\n  cnn1.weight_fn = \"/w01.txt\";\n  cnn1.bias_fn   = \"/b01.txt\";\n\n  Conv cnn2;\n  cnn2.K = 5;\n  cnn2.P = 0;\n  cnn2.S = 1; // valid padding\n  cnn2.weight_fn = \"/w02.txt\";\n  cnn2.bias_fn   = \"/b02.txt\";\n\n  Pool pool;\n  pool.M = 2;\n  pool.T = 2;\n\n  FCNMem fcn_mem1;\n  fcn_mem1.weight = w03;\n  fcn_mem1.bias   = b03;\n  fcn_mem1.act    = ACT_RELU;\n\n  FCNMem fcn_mem2;\n  fcn_mem2.weight = w04;\n  fcn_mem2.bias   = b04;\n  fcn_mem2.act    = ACT_RELU;\n\n  FCNMem fcn_mem3;\n  fcn_mem3.weight = w05;\n  fcn_mem3.bias   = b05;\n  fcn_mem3.act    = ACT_SOFTMAX;\n\n  unsigned long st = micros();\n  uint16_t V;\n\n  V = noodle_conv_float(BUFFER1, 1, 6, BUFFER3, 28, cnn1, pool, NULL);\n  V = noodle_conv_float(BUFFER3, 6, 16, BUFFER1, V, cnn2, pool, NULL);\n\n  V = noodle_flat(BUFFER1, BUFFER3, V, 16);\n\n  V = noodle_fcn(BUFFER3, V, 120, BUFFER1, fcn_mem1, NULL);\n  V = noodle_fcn(BUFFER1, V, 84,  BUFFER3, fcn_mem2, NULL);\n  V = noodle_fcn(BUFFER3, V, 10,  BUFFER1, fcn_mem3, NULL);\n  \u22ee\n  \u22ee\n</code></pre> <p></p>"},{"location":"lenet-5-pico/","title":"LeNet-5 on Raspberry Pico","text":"<p>Repository: https://github.com/auralius/noodle/tree/main/examples/lenet-5-esp32  Select the <code>[env:rpi_pico]</code> environment. </p> <p>In the <code>test</code> folder, there is a Python script, named <code>mnist-sender.py</code>. This Python scripts reads a line from the MNIST dataset (\\(28 \\times 28=784\\) bytes) and streams the data to the Pico.</p> <p>Raspberry Pico does not support partitioning of its internal flash as in ESP32. Therefore, both CNN and FCN are implemented as variables. The layer-by-layer implementation is exactly identical as in ESP32.</p> <p></p> <p>The inference time stays consistently at ~1 second.</p>"},{"location":"mobile-lenet-5/","title":"Mobile-LeNet","text":"<ul> <li>Google Colab notebook can be found here.</li> <li>Noodle Implementation can be found here.</li> </ul>"},{"location":"mobile-lenet-5/#design-choices","title":"Design Choices","text":"<p>Mobile-LeNet is a compact convolutional neural network inspired by both LeNet-5 and MobileNet-V1. Its purpose is didactic rather than competitive: to expose, in a single small model, the full set of modern CNN building blocks commonly used in edge and embedded deep learning. Specifically, this model demonstrates:</p> <ul> <li>Standard convolution</li> <li>Depth-wise convolution    </li> <li>Point-wise (1\u00d71) convolution</li> <li>Batch Normalization</li> <li>Non-linear activation (ReLU)</li> <li>Spatial downsampling via stride</li> <li>Global Average Pooling (GAP)</li> <li>Dense classification (or fully connected network / FCN)</li> </ul> <p>The target dataset is MNIST (28\u00d728 grayscale)</p> <p></p>"},{"location":"mobile-lenet-5/#architectural-overview","title":"Architectural Overview","text":"# Layer (Type) Output Shape Params Weight / Bias file 1 <code>stem_conv3x3</code> (Conv2D) 28\u00d728\u00d78 80 <code>w01.txt</code>, <code>b01.txt</code> 2 <code>stem_bn</code> (BatchNorm) 28\u00d728\u00d78 32 <code>bn01.txt</code> 3 <code>stem_relu</code> (ReLU) 28\u00d728\u00d78 0 \u2014 4 <code>B1_dw3x3_s1</code> (DepthwiseConv2D) 28\u00d728\u00d78 72 <code>w02.txt</code> 5 <code>B1_dw_bn</code> (BatchNorm) 28\u00d728\u00d78 32 <code>bn02.txt</code> 6 <code>B1_dw_relu</code> (ReLU) 28\u00d728\u00d78 0 \u2014 7 <code>B1_pw1x1</code> (Conv2D) 28\u00d728\u00d78 64 <code>w03.txt</code> 8 <code>B1_pw_bn</code> (BatchNorm) 28\u00d728\u00d78 32 <code>bn03.txt</code> 9 <code>B1_pw_relu</code> (ReLU) 28\u00d728\u00d78 0 \u2014 10 <code>B2_dw3x3_s1</code> (DepthwiseConv2D) 28\u00d728\u00d78 72 <code>w04.txt</code> 11 <code>B2_dw_bn</code> (BatchNorm) 28\u00d728\u00d78 32 <code>bn04.txt</code> 12 <code>B2_dw_relu</code> (ReLU) 28\u00d728\u00d78 0 \u2014 13 <code>B2_pw1x1</code> (Conv2D) 28\u00d728\u00d78 64 <code>w05.txt</code> 14 <code>B2_pw_bn</code> (BatchNorm) 28\u00d728\u00d78 32 <code>bn05.txt</code> 15 <code>B2_pw_relu</code> (ReLU) 28\u00d728\u00d78 0 \u2014 16 <code>B3_pad1</code> (ZeroPadding2D) 30\u00d730\u00d78 0 \u2014 17 <code>B3_dw3x3_s2</code> (DepthwiseConv2D) 14\u00d714\u00d78 72 <code>w06.txt</code> 18 <code>B3_dw_bn</code> (BatchNorm) 14\u00d714\u00d78 32 <code>bn06.txt</code> 19 <code>B3_dw_relu</code> (ReLU) 14\u00d714\u00d78 0 \u2014 20 <code>B3_pw1x1</code> (Conv2D) 14\u00d714\u00d716 128 <code>w07.txt</code> 21 <code>B3_pw_bn</code> (BatchNorm) 14\u00d714\u00d716 64 <code>bn07.txt</code> 22 <code>B3_pw_relu</code> (ReLU) 14\u00d714\u00d716 0 \u2014 23 <code>B4_dw3x3_s1</code> (DepthwiseConv2D) 14\u00d714\u00d716 144 <code>w08.txt</code> 24 <code>B4_dw_bn</code> (BatchNorm) 14\u00d714\u00d716 64 <code>bn08.txt</code> 25 <code>B4_dw_relu</code> (ReLU) 14\u00d714\u00d716 0 \u2014 26 <code>B4_pw1x1</code> (Conv2D) 14\u00d714\u00d716 256 <code>w09.txt</code> 27 <code>B4_pw_bn</code> (BatchNorm) 14\u00d714\u00d716 64 <code>bn09.txt</code> 28 <code>B4_pw_relu</code> (ReLU) 14\u00d714\u00d716 0 \u2014 29 <code>B5_pad1</code> (ZeroPadding2D) 16\u00d716\u00d716 0 \u2014 30 <code>B5_dw3x3_s2</code> (DepthwiseConv2D) 7\u00d77\u00d716 144 <code>w10.txt</code> 31 <code>B5_dw_bn</code> (BatchNorm) 7\u00d77\u00d716 64 <code>bn10.txt</code> 32 <code>B5_dw_relu</code> (ReLU) 7\u00d77\u00d716 0 \u2014 33 <code>B5_pw1x1</code> (Conv2D) 7\u00d77\u00d724 384 <code>w11.txt</code> 34 <code>B5_pw_bn</code> (BatchNorm) 7\u00d77\u00d724 96 <code>bn11.txt</code> 35 <code>B5_pw_relu</code> (ReLU) 7\u00d77\u00d724 0 \u2014 36 <code>B6_dw3x3_s1</code> (DepthwiseConv2D) 7\u00d77\u00d724 216 <code>w12.txt</code> 37 <code>B6_dw_bn</code> (BatchNorm) 7\u00d77\u00d724 96 <code>bn12.txt</code> 38 <code>B6_dw_relu</code> (ReLU) 7\u00d77\u00d724 0 \u2014 39 <code>B6_pw1x1</code> (Conv2D) 7\u00d77\u00d724 576 <code>w13.txt</code> 40 <code>B6_pw_bn</code> (BatchNorm) 7\u00d77\u00d724 96 <code>bn13.txt</code> 41 <code>B6_pw_relu</code> (ReLU) 7\u00d77\u00d724 0 \u2014 42 <code>GAP</code> (GlobalAveragePooling2D) 24 0 \u2014 43 <code>OUT</code> (Dense) 10 250 <code>w14.txt</code>, <code>b02.txt</code> Total parameters: 3,258Trainable parameters: 2,890Non-trainable parameters: 368 <p>The network above follows a progressive feature extraction strategy:</p> <ol> <li>Early layers preserve spatial resolution and learn low-level features (edges, strokes).</li> <li>Intermediate layers downsample while increasing channel capacity.</li> <li>Late layers aggregate global structure before classification.</li> </ol> <p>Downsampling is performed using stride-2 depth-wise convolution, replacing the pooling layers used in classic LeNet.</p>"},{"location":"mobile-lenet-5/#stem-convolution","title":"Stem Convolution","text":"<p>The stem consists of a single standard convolution:</p> <ul> <li>K = 3\u00d73, S = 1, P = 1</li> <li>Expands the channel dimension from 1 \u2192 8</li> <li>Preserves spatial resolution (28\u00d728)</li> </ul> <p>This layer converts raw pixel intensity into a small set of learned feature maps. Batch Normalization and ReLU immediately follow to stabilize optimization and introduce nonlinearity.</p>"},{"location":"mobile-lenet-5/#depth-wise-separable-blocks-dw-pw","title":"Depth-wise Separable Blocks (DW \u2192 PW)","text":"<p>Each block consists of two stages:</p>"},{"location":"mobile-lenet-5/#1-depth-wise-convolution-dwconv","title":"1. Depth-wise Convolution (DWConv)","text":"<ul> <li>Operates independently on each channel</li> <li>Learns spatial filters without mixing channels</li> <li>Computational cost scales with spatial size only</li> </ul>"},{"location":"mobile-lenet-5/#2-point-wise-convolution-pwconv-11","title":"2. Point-wise Convolution (PWConv, 1\u00d71)","text":"<ul> <li>Mixes information across channels</li> <li>Controls the number of output feature maps</li> <li>Restores representational power lost by depth-wise separation</li> </ul> <p>This separation dramatically reduces multiply-accumulate operations (MACs) compared to standard convolutions, making the architecture suitable for embedded inference.</p>"},{"location":"mobile-lenet-5/#stage-wise-organization","title":"Stage-wise Organization","text":""},{"location":"mobile-lenet-5/#stage-1-feature-refinement-2828","title":"Stage 1 \u2014 Feature Refinement (28\u00d728)","text":"<p>Blocks B1\u2013B2 operate at full spatial resolution:</p> <ul> <li>Stride S = 1</li> <li>No downsampling</li> <li>Focus on stabilizing and refining low-level features</li> </ul> <p>This mirrors early layers in LeNet, where preserving detail is critical for thin digit strokes.</p>"},{"location":"mobile-lenet-5/#stage-2-first-downsampling-1414","title":"Stage 2 \u2014 First Downsampling (14\u00d714)","text":"<p>Block B3 introduces downsampling:</p> <ul> <li>Depth-wise convolution with S = 2</li> <li>Explicit padding + VALID convolution</li> <li>Channel expansion: 8 \u2192 16</li> </ul> <p>Block B4 then refines features at the new scale. Downsampling is delayed until sufficient local structure has been extracted, reducing information loss.</p>"},{"location":"mobile-lenet-5/#stage-3-second-downsampling-77","title":"Stage 3 \u2014 Second Downsampling (7\u00d77)","text":"<p>Block B5 performs a second downsampling:</p> <ul> <li>Depth-wise convolution with S = 2 </li> <li>Channel expansion: 16 \u2192 24</li> </ul> <p>Block B6 refines global digit structure before classification. At this stage, each feature map responds to large regions of the input image.</p>"},{"location":"mobile-lenet-5/#padding-strategy","title":"Padding Strategy","text":"<p>Padding is explicitly controlled and expressed in terms of <code>(K, S, P)</code> rather than abstract <code>\"same\"</code> semantics.</p> <ul> <li>For S = 1, symmetric padding with <code>P = 1</code> preserves spatial size.</li> <li>For S = 2, padding is applied explicitly before a VALID convolution to control output dimensions deterministically.</li> </ul>"},{"location":"mobile-lenet-5/#global-average-pooling","title":"Global Average Pooling","text":"<p>Instead of a fully connected spatial flattening:</p> <ul> <li>Global Average Pooling (GAP) collapses each 7\u00d77 feature map into a single scalar</li> <li>Output dimension becomes 24</li> </ul> <p>Advantages:</p> <ul> <li>Reduces parameter count</li> <li>Improves translation robustness</li> <li>Avoids large dense layers (important for MCUs)</li> </ul>"},{"location":"mobile-lenet-5/#classification-layer","title":"Classification Layer","text":"<p>A final dense layer maps: - 24 \u2192 10 outputs - Followed by softmax for digit classification</p> <p>This mirrors the role of the final fully connected layers in LeNet-5, but with far fewer parameters.</p>"},{"location":"mobile-lenet-5/#noodle-implementation","title":"Noodle Implementation","text":"<p>One set of MobileNet consists of:</p> <ul> <li>Depth-wise convolution (DW)</li> <li>Batch normalization (BN)</li> <li>ReLU</li> <li>Point-wise convolution (PW)</li> <li>Batch normalization (BN)</li> <li>ReLU</li> </ul> <p>Therefore, we create a helper function <code>noodle_dw_pw_block(...)</code> that runs this sequence once: DW \u2192 BN \u2192 ReLU \u2192 PW \u2192 BN \u2192 ReLU.</p> <pre><code>uint16_t noodle_dw_pw_block(float *in, float *out,\n                            uint16_t W_in,\n                            uint16_t Cin,\n                            uint16_t Cout,\n                            uint16_t stride_dw,\n                            const float *w_dw, const float *bn_dw,\n                            const float *w_pw, const float *bn_pw)\n{\n    const float BN_EPS = 1e-3f;   // match Keras default\n    Pool none{}; // M=1,T=1 by default\n\n    // DW (Cin -&gt; Cin)\n    ConvMem dw{}; \n    dw.K = 3; dw.P = 1; dw.S = stride_dw;\n    dw.weight = w_dw; \n    dw.bias   = nullptr; \n    dw.act    = ACT_NONE;\n    uint16_t W = noodle_dwconv_float(in, Cin, out, W_in, dw, none, nullptr);\n    noodle_bn_relu(out, Cin, W, bn_dw, BN_EPS);  \n\n    // PW (Cin -&gt; Cout)\n    ConvMem pw{}; \n    pw.K = 1; pw.P = 0; pw.S = 1;\n    pw.weight = w_pw; \n    pw.bias   = nullptr; \n    pw.act    = ACT_NONE;\n    W = noodle_conv_float(out, Cin, Cout, in, W, pw, none, nullptr);\n    noodle_bn_relu(in, Cout, W, bn_pw, BN_EPS);\n    return W;\n}\n\nvoid predict()\n{\n    \u22ee\n    \u22ee   \n    // ---- Input ----\n    // FEAT_A holds input image in CHW where C=1, W=28: [1][28][28]\n\n    // ---- No Pooling ----\n    Pool none{}; none.M = 1; none.T = 1;\n\n    // ---- Stem: Conv3x3 (1-&gt;8) + BN + ReLU ----\n    ConvMem stem{};\n    stem.K = 3; stem.P = 1; stem.S = 1;\n    stem.weight = w01;\n    stem.bias   = b01;\n    stem.act    = ACT_NONE;\n\n    // ---- Dense: 24 -&gt; 10 ----\n    FCNMem head{};\n    head.weight = w14;   // row-major [10,24]\n    head.bias   = b02;   // 10\n    head.act    = ACT_NONE;\n\n    uint16_t W = noodle_conv_float(FEAT_A, 1, 8, FEAT_B, 28, stem, none, nullptr);  \n    noodle_bn_relu(FEAT_B, 8, W, bn01, 1e-3f);\n\n    // Ping-pong buffers\n    float *in  = FEAT_B;\n    float *out = FEAT_A;\n\n    // ---- B1 (8-&gt;8, stride 1) ----\n    W = noodle_dw_pw_block(in, out, W, 8, 8, 1, w02, bn02, w03, bn03);\n    // ---- B2 (8-&gt;8, stride 1) ----\n    W = noodle_dw_pw_block(in, out, W, 8, 8, 1, w04, bn04, w05, bn05);\n    // ---- B3 (8-&gt;16, stride 2) ----\n    W = noodle_dw_pw_block(in, out, W, 8, 16, 2, w06, bn06, w07, bn07);\n    // ---- B4 (16-&gt;16, stride 1) ----\n    W = noodle_dw_pw_block(in, out, W, 16, 16, 1, w08, bn08, w09, bn09);\n    // ---- B5 (16-&gt;24, stride 2) ----\n    W = noodle_dw_pw_block(in, out, W, 16, 24, 2, w10, bn10, w11, bn11);\n    // ---- B6 (24-&gt;24, stride 1) ----\n    W = noodle_dw_pw_block(in, out, W, 24, 24, 1, w12, bn12, w13, bn13);\n    // ---- GAP: (W x W x 24) -&gt; (24,) in-place ----\n    W = noodle_gap(in, 24, W);\n    // ---- Dense: (24,) -&gt; (10,) ----\n    W = noodle_fcn(in, W, 10, out, head, nullptr);\n    // Softmax in-place on logits\n    noodle_soft_max(out, 10);\n    \u22ee\n    \u22ee\n}\n</code></pre>"},{"location":"usps-fcn-uno/","title":"Two layer fully connected network on ATmega328 (Arduino Uno R3)","text":""},{"location":"usps-fcn-uno/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>GitHub (Visual Code and PlatformIO).</li> <li>Google Colab notebook </li> <li>Dataset link</li> </ul> </li> <li>Video demonstration : (speed x 2) </li> </ul>"},{"location":"usps-fcn-uno/#plan","title":"Plan","text":"<p>This project demonstrates how an SD card can be used to implement a two-layer Fully Connected Network (FCN) for digit recognition on an Arduino Uno R3.  </p> <p>The Arduino Uno R3 has extremely limited RAM and flash memory. In practical applications, especially those involving a graphical user interface, the interface alone can consume a large portion of the available resources. This makes it impractical to store intermediate activations, layer outputs, or large parameter arrays directly in memory.  </p> <p>To address this limitation, our plan is to rely on external storage in the form of an SD card. We will use the SD card to stream data that would otherwise exceed the memory capacity of the microcontroller, enabling the execution of a simple neural network pipeline under tight embedded constraints at the cost of the inference time.</p>"},{"location":"usps-fcn-uno/#hardware","title":"Hardware","text":"<ul> <li>Arduino Uno R3</li> <li>2.4 in TFT touch display (UNO shield) from MCUFRIEND</li> <li>SD Card</li> </ul>"},{"location":"usps-fcn-uno/#training","title":"Training","text":"<p>The network is designed and trained in Google Colab using the USPS dataset. The USPS dataset is selected over MNIST to minimize memory requirements. Because inference is executed directly on the ATmega328 microcontroller, strict memory constraints are encountered as early as the preprocessing stage. This is due to the device\u2019s 2 KB RAM capacity.</p> <p></p> <p>The following Python code shows the Keras implementation. Here, we set 64 neurons (\\(n=64\\)) in the hidden layer.</p> <pre><code>model = Sequential([\n    Dense(64, input_shape=(256,), activation='relu', kernel_initializer=initializer),  \n    Dropout(0.3),  # &lt;-- training only\n    Dense(10, activation='softmax', kernel_initializer=initializer)  #\n], name='two-layer-fcn')\n</code></pre> <p>After training, we can run the exporter and will receive the following files. These file will be copied to the SD Card.</p> <pre><code>/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w01.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w01.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b01.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b01.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w02.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w02.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b02.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b02.h\n</code></pre> Layer Input Output Weight files Bias files Location Fully connected #1 256 64 <code>w01.txt</code> <code>b01.txt</code> SD Card Fully connected #2 64 10 <code>w02.txt</code> <code>b02.txt</code> SD Card"},{"location":"usps-fcn-uno/#on-the-uno-side","title":"On the Uno side","text":"<pre><code>FCNFile FCN1;\nFCN1.weight_fn = \"w01.txt\";\nFCN1.bias_fn = \"b01.txt\";\nFCN1.act = ACT_RELU;\n// 256 input neurons, 64 hidden neurons\nuint16_t V = noodle_fcn(GRID, 256, 64, OUTPUT_BUFFER1, FCN1, progress_hnd);\n\nFCNFile FCN2;\nFCN2.weight_fn = \"w02.txt\";\nFCN2.bias_fn = \"b02.txt\";\nFCN2.act = ACT_SOFTMAX;\n// 10 output neurons\nV = noodle_fcn(OUTPUT_BUFFER1, V, 10, OUTPUT_BUFFER2, FCN2, progress_hnd);\n</code></pre> <p>The inference completes in  about 10 to 11 seconds. This timing is very consistent since variability only comes from data transfer from SD Card to the micrcontroller.</p>"},{"location":"vww-esp32/","title":"Visual Wake Word on ESP 32","text":""},{"location":"vww-esp32/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>GitHub (Visual Code and PlatformIO).</li> </ul> </li> </ul> <p>This example is based on the MLPerf Tiny Visual Wake Word (VWW) benchmark by using COCO2014 dataset. The classification task in to predict if there is person(s) in the image (the \u201cperson vs non-person\u201d benchmark). The paper can be found here.</p>"},{"location":"vww-esp32/#mobilenetv1","title":"MobileNetV1","text":"<p>The architecture is based on the MLPerf Tiny paper, which is slightly different than the original MobileNetV1 paper.</p> Layer # Type S P Weight file (dims) Bias file (dims) 01 Conv 3\u00d73 (Cin=3, Cout=8) 2 1 <code>w01.txt</code> (3\u00d73\u00d73\u00d78) <code>b01.txt</code> (8) 02 Depthwise 3\u00d73 (Cin=8, M=1) 1 1 <code>w02.txt</code> (3\u00d73\u00d78) <code>b02.txt</code> (8) 03 Conv 1\u00d71 (Cin=8, Cout=16) 1 0 <code>w03.txt</code> (1\u00d71\u00d78\u00d716) <code>b03.txt</code> (16) 04 Depthwise 3\u00d73 (Cin=16, M=1) 2 1 <code>w04.txt</code> (3\u00d73\u00d716) <code>b04.txt</code> (16) 05 Conv 1\u00d71 (Cin=16, Cout=32) 1 0 <code>w05.txt</code> (1\u00d71\u00d716\u00d732) <code>b05.txt</code> (32) 06 Depthwise 3\u00d73 (Cin=32, M=1) 1 1 <code>w06.txt</code> (3\u00d73\u00d732) <code>b06.txt</code> (32) 07 Conv 1\u00d71 (Cin=32, Cout=32) 1 0 <code>w07.txt</code> (1\u00d71\u00d732\u00d732) <code>b07.txt</code> (32) 08 Depthwise 3\u00d73 (Cin=32, M=1) 2 1 <code>w08.txt</code> (3\u00d73\u00d732) <code>b08.txt</code> (32) 09 Conv 1\u00d71 (Cin=32, Cout=64) 1 0 <code>w09.txt</code> (1\u00d71\u00d732\u00d764) <code>b09.txt</code> (64) 10 Depthwise 3\u00d73 (Cin=64, M=1) 1 1 <code>w10.txt</code> (3\u00d73\u00d764) <code>b10.txt</code> (64) 11 Conv 1\u00d71 (Cin=64, Cout=64) 1 0 <code>w11.txt</code> (1\u00d71\u00d764\u00d764) <code>b11.txt</code> (64) 12 Depthwise 3\u00d73 (Cin=64, M=1) 2 1 <code>w12.txt</code> (3\u00d73\u00d764) <code>b12.txt</code> (64) 13 Conv 1\u00d71 (Cin=64, Cout=128) 1 0 <code>w13.txt</code> (1\u00d71\u00d764\u00d7128) <code>b13.txt</code> (128) 14 Depthwise 3\u00d73 (Cin=128, M=1) 1 1 <code>w14.txt</code> (3\u00d73\u00d7128) <code>b14.txt</code> (128) 15 Conv 1\u00d71 (Cin=128, Cout=128) 1 0 <code>w15.txt</code> (1\u00d71\u00d7128\u00d7128) <code>b15.txt</code> (128) 16 Depthwise 3\u00d73 (Cin=128, M=1) 1 1 <code>w16.txt</code> (3\u00d73\u00d7128) <code>b16.txt</code> (128) 17 Conv 1\u00d71 (Cin=128, Cout=128) 1 0 <code>w17.txt</code> (1\u00d71\u00d7128\u00d7128) <code>b17.txt</code> (128) 18 Depthwise 3\u00d73 (Cin=128, M=1) 1 1 <code>w18.txt</code> (3\u00d73\u00d7128) <code>b18.txt</code> (128) 19 Conv 1\u00d71 (Cin=128, Cout=128) 1 0 <code>w19.txt</code> (1\u00d71\u00d7128\u00d7128) <code>b19.txt</code> (128) 20 Depthwise 3\u00d73 (Cin=128, M=1) 1 1 <code>w20.txt</code> (3\u00d73\u00d7128) <code>b20.txt</code> (128) 21 Conv 1\u00d71 (Cin=128, Cout=128) 1 0 <code>w21.txt</code> (1\u00d71\u00d7128\u00d7128) <code>b21.txt</code> (128) 22 Depthwise 3\u00d73 (Cin=128, M=1) 1 1 <code>w22.txt</code> (3\u00d73\u00d7128) <code>b22.txt</code> (128) 23 Conv 1\u00d71 (Cin=128, Cout=128) 1 0 <code>w23.txt</code> (1\u00d71\u00d7128\u00d7128) <code>b23.txt</code> (128) 24 Depthwise 3\u00d73 (Cin=128, M=1) 2 1 <code>w24.txt</code> (3\u00d73\u00d7128) <code>b24.txt</code> (128) 25 Conv 1\u00d71 (Cin=128, Cout=256) 1 0 <code>w25.txt</code> (1\u00d71\u00d7128\u00d7256) <code>b25.txt</code> (256) 26 Depthwise 3\u00d73 (Cin=256, M=1) 1 1 <code>w26.txt</code> (3\u00d73\u00d7256) <code>b26.txt</code> (256) 27 Conv 1\u00d71 (Cin=256, Cout=256) 1 0 <code>w27.txt</code> (1\u00d71\u00d7256\u00d7256) <code>b27.txt</code> (256) 28 FC (in=256, out=2) \u2013 \u2013 <code>w28.txt</code> (2\u00d7256) <code>b28.txt</code> (2) <pre><code>Op 00: CONV_2D\n  inputs : [(0, [1, 96, 96, 3]), (44, [8, 3, 3, 3]), (3, [8])]\n  outputs: [(58, [1, 48, 48, 8])]\n  stride : (2, 2)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 01: DEPTHWISE_CONV_2D\n  inputs : [(58, [1, 48, 48, 8]), (5, [1, 3, 3, 8]), (4, [8])]\n  outputs: [(59, [1, 48, 48, 8])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 02: CONV_2D\n  inputs : [(59, [1, 48, 48, 8]), (45, [16, 1, 1, 8]), (21, [16])]\n  outputs: [(60, [1, 48, 48, 16])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 03: DEPTHWISE_CONV_2D\n  inputs : [(60, [1, 48, 48, 16]), (33, [1, 3, 3, 16]), (32, [16])]\n  outputs: [(61, [1, 24, 24, 16])]\n  stride : (2, 2)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 04: CONV_2D\n  inputs : [(61, [1, 24, 24, 16]), (46, [32, 1, 1, 16]), (34, [32])]\n  outputs: [(62, [1, 24, 24, 32])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 05: DEPTHWISE_CONV_2D\n  inputs : [(62, [1, 24, 24, 32]), (36, [1, 3, 3, 32]), (35, [32])]\n  outputs: [(63, [1, 24, 24, 32])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 06: CONV_2D\n  inputs : [(63, [1, 24, 24, 32]), (47, [32, 1, 1, 32]), (37, [32])]\n  outputs: [(64, [1, 24, 24, 32])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 07: DEPTHWISE_CONV_2D\n  inputs : [(64, [1, 24, 24, 32]), (39, [1, 3, 3, 32]), (38, [32])]\n  outputs: [(65, [1, 12, 12, 32])]\n  stride : (2, 2)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 08: CONV_2D\n  inputs : [(65, [1, 12, 12, 32]), (48, [64, 1, 1, 32]), (40, [64])]\n  outputs: [(66, [1, 12, 12, 64])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 09: DEPTHWISE_CONV_2D\n  inputs : [(66, [1, 12, 12, 64]), (42, [1, 3, 3, 64]), (41, [64])]\n  outputs: [(67, [1, 12, 12, 64])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 10: CONV_2D\n  inputs : [(67, [1, 12, 12, 64]), (49, [64, 1, 1, 64]), (6, [64])]\n  outputs: [(68, [1, 12, 12, 64])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 11: DEPTHWISE_CONV_2D\n  inputs : [(68, [1, 12, 12, 64]), (8, [1, 3, 3, 64]), (7, [64])]\n  outputs: [(69, [1, 6, 6, 64])]\n  stride : (2, 2)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 12: CONV_2D\n  inputs : [(69, [1, 6, 6, 64]), (50, [128, 1, 1, 64]), (9, [128])]\n  outputs: [(70, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 13: DEPTHWISE_CONV_2D\n  inputs : [(70, [1, 6, 6, 128]), (11, [1, 3, 3, 128]), (10, [128])]\n  outputs: [(71, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 14: CONV_2D\n  inputs : [(71, [1, 6, 6, 128]), (51, [128, 1, 1, 128]), (12, [128])]\n  outputs: [(72, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 15: DEPTHWISE_CONV_2D\n  inputs : [(72, [1, 6, 6, 128]), (14, [1, 3, 3, 128]), (13, [128])]\n  outputs: [(73, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 16: CONV_2D\n  inputs : [(73, [1, 6, 6, 128]), (52, [128, 1, 1, 128]), (15, [128])]\n  outputs: [(74, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 17: DEPTHWISE_CONV_2D\n  inputs : [(74, [1, 6, 6, 128]), (17, [1, 3, 3, 128]), (16, [128])]\n  outputs: [(75, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 18: CONV_2D\n  inputs : [(75, [1, 6, 6, 128]), (53, [128, 1, 1, 128]), (18, [128])]\n  outputs: [(76, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 19: DEPTHWISE_CONV_2D\n  inputs : [(76, [1, 6, 6, 128]), (20, [1, 3, 3, 128]), (19, [128])]\n  outputs: [(77, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 20: CONV_2D\n  inputs : [(77, [1, 6, 6, 128]), (54, [128, 1, 1, 128]), (22, [128])]\n  outputs: [(78, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 21: DEPTHWISE_CONV_2D\n  inputs : [(78, [1, 6, 6, 128]), (24, [1, 3, 3, 128]), (23, [128])]\n  outputs: [(79, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 22: CONV_2D\n  inputs : [(79, [1, 6, 6, 128]), (55, [128, 1, 1, 128]), (25, [128])]\n  outputs: [(80, [1, 6, 6, 128])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 23: DEPTHWISE_CONV_2D\n  inputs : [(80, [1, 6, 6, 128]), (27, [1, 3, 3, 128]), (26, [128])]\n  outputs: [(81, [1, 3, 3, 128])]\n  stride : (2, 2)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 24: CONV_2D\n  inputs : [(81, [1, 3, 3, 128]), (56, [256, 1, 1, 128]), (28, [256])]\n  outputs: [(82, [1, 3, 3, 256])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 25: DEPTHWISE_CONV_2D\n  inputs : [(82, [1, 3, 3, 256]), (30, [1, 3, 3, 256]), (29, [256])]\n  outputs: [(83, [1, 3, 3, 256])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  depth_multiplier: 1\n  act    : 1\n\nOp 26: CONV_2D\n  inputs : [(83, [1, 3, 3, 256]), (57, [256, 1, 1, 256]), (31, [256])]\n  outputs: [(84, [1, 3, 3, 256])]\n  stride : (1, 1)\n  padding: SAME\n  dilation: (1, 1)\n  act    : 1\n\nOp 27: AVERAGE_POOL_2D\n  inputs : [(84, [1, 3, 3, 256])]\n  outputs: [(85, [1, 1, 1, 256])]\n  stride : (3, 3)\n  filter : (3, 3)\n  padding: VALID\n  act    : 0\n\nOp 28: BUILTIN_22\n  inputs : [(85, [1, 1, 1, 256]), (2, [2])]\n  outputs: [(86, [1, 256])]\n\nOp 29: FULLY_CONNECTED\n  inputs : [(86, [1, 256]), (43, [2, 256]), (1, [2])]\n  outputs: [(87, [1, 2])]\n\nOp 30: BUILTIN_25\n  inputs : [(87, [1, 2])]\n  outputs: [(88, [1, 2])]\n</code></pre>"},{"location":"vww-esp32/#extract-benchmark-data","title":"Extract benchmark data","text":"<p>MLPerf Tiny has already provided <code>.tflite</code> file. Therefore, we will first extract all weights and biases by using the following python code. We then put all these files to <code>data</code>  folder of the VS Code / PlatformIO  working directory.</p>"},{"location":"vww-esp32/#layer-by-layer-implementation-on-esp32","title":"Layer-by-Layer Implementation on ESP32","text":"<p>The following C++ code demonstrates the \"Noodle\" layer-by-layer implementation for the ESP32:</p> <pre><code>Pool POOL_ID; POOL_ID.M = 1; POOL_ID.T = 1;\n\nuint16_t W = IN_W;\nuint16_t V = 0;\n\nConv c00; c00.K=3; c00.P=1; c00.S=2; c00.weight_fn=W_OP00; c00.bias_fn=B_OP00; c00.act=ACT_RELU;\nV = noodle_conv_float(IN, 3, 8, A, W, c00, POOL_ID, nullptr); W = V;\n\nConv d01; d01.K=3; d01.P=1; d01.S=1; d01.weight_fn=W_OP01; d01.bias_fn=B_OP01; d01.act=ACT_RELU;\nV = noodle_dwconv_float(A, 8, B, W, d01, POOL_ID, nullptr); W = V;\n\nConv c02; c02.K=1; c02.P=0; c02.S=1; c02.weight_fn=W_OP02; c02.bias_fn=B_OP02; c02.act=ACT_RELU;\nV = noodle_conv_float(B, 8, 16, A, W, c02, POOL_ID, nullptr); W = V;\n\nConv d03; d03.K=3; d03.P=1; d03.S=2; d03.weight_fn=W_OP03; d03.bias_fn=B_OP03; d03.act=ACT_RELU;\nV = noodle_dwconv_float(A, 16, B, W, d03, POOL_ID, nullptr); W = V;\n\nConv c04; c04.K=1; c04.P=0; c04.S=1; c04.weight_fn=W_OP04; c04.bias_fn=B_OP04; c04.act=ACT_RELU;\nV = noodle_conv_float(B, 16, 32, A, W, c04, POOL_ID, nullptr); W = V;\n\nConv d05; d05.K=3; d05.P=1; d05.S=1; d05.weight_fn=W_OP05; d05.bias_fn=B_OP05; d05.act=ACT_RELU;\nV = noodle_dwconv_float(A, 32, B, W, d05, POOL_ID, nullptr); W = V;\n\nConv c06; c06.K=1; c06.P=0; c06.S=1; c06.weight_fn=W_OP06; c06.bias_fn=B_OP06; c06.act=ACT_RELU;\nV = noodle_conv_float(B, 32, 32, A, W, c06, POOL_ID, nullptr); W = V;\n\nConv d07; d07.K=3; d07.P=1; d07.S=2; d07.weight_fn=W_OP07; d07.bias_fn=B_OP07; d07.act=ACT_RELU;\nV = noodle_dwconv_float(A, 32, B, W, d07, POOL_ID, nullptr); W = V;\n\nConv c08; c08.K=1; c08.P=0; c08.S=1; c08.weight_fn=W_OP08; c08.bias_fn=B_OP08; c08.act=ACT_RELU;\nV = noodle_conv_float(B, 32, 64, A, W, c08, POOL_ID, nullptr); W = V;\n\nConv d09; d09.K=3; d09.P=1; d09.S=1; d09.weight_fn=W_OP09; d09.bias_fn=B_OP09; d09.act=ACT_RELU;\nV = noodle_dwconv_float(A, 64, B, W, d09, POOL_ID, nullptr); W = V;\n\nConv c10; c10.K=1; c10.P=0; c10.S=1; c10.weight_fn=W_OP10; c10.bias_fn=B_OP10; c10.act=ACT_RELU;\nV = noodle_conv_float(B, 64, 64, A, W, c10, POOL_ID, nullptr); W = V;\n\nConv d11; d11.K=3; d11.P=1; d11.S=2; d11.weight_fn=W_OP11; d11.bias_fn=B_OP11; d11.act=ACT_RELU;\nV = noodle_dwconv_float(A, 64, B, W, d11, POOL_ID, nullptr); W = V;\n\nConv c12; c12.K=1; c12.P=0; c12.S=1; c12.weight_fn=W_OP12; c12.bias_fn=B_OP12; c12.act=ACT_RELU;\nV = noodle_conv_float(B, 64, 128, A, W, c12, POOL_ID, nullptr); W = V;\n\nConv d13; d13.K=3; d13.P=1; d13.S=1; d13.weight_fn=W_OP13; d13.bias_fn=B_OP13; d13.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d13, POOL_ID, nullptr); W = V;\n\nConv c14; c14.K=1; c14.P=0; c14.S=1; c14.weight_fn=W_OP14; c14.bias_fn=B_OP14; c14.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 128, A, W, c14, POOL_ID, nullptr); W = V;\n\nConv d15; d15.K=3; d15.P=1; d15.S=1; d15.weight_fn=W_OP15; d15.bias_fn=B_OP15; d15.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d15, POOL_ID, nullptr); W = V;\n\nConv c16; c16.K=1; c16.P=0; c16.S=1; c16.weight_fn=W_OP16; c16.bias_fn=B_OP16; c16.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 128, A, W, c16, POOL_ID, nullptr); W = V;\n\nConv d17; d17.K=3; d17.P=1; d17.S=1; d17.weight_fn=W_OP17; d17.bias_fn=B_OP17; d17.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d17, POOL_ID, nullptr); W = V;\n\nConv c18; c18.K=1; c18.P=0; c18.S=1; c18.weight_fn=W_OP18; c18.bias_fn=B_OP18; c18.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 128, A, W, c18, POOL_ID, nullptr); W = V;\n\nConv d19; d19.K=3; d19.P=1; d19.S=1; d19.weight_fn=W_OP19; d19.bias_fn=B_OP19; d19.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d19, POOL_ID, nullptr); W = V;\n\nConv c20; c20.K=1; c20.P=0; c20.S=1; c20.weight_fn=W_OP20; c20.bias_fn=B_OP20; c20.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 128, A, W, c20, POOL_ID, nullptr); W = V;\n\nConv d21; d21.K=3; d21.P=1; d21.S=1; d21.weight_fn=W_OP21; d21.bias_fn=B_OP21; d21.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d21, POOL_ID, nullptr); W = V;\n\nConv c22; c22.K=1; c22.P=0; c22.S=1; c22.weight_fn=W_OP22; c22.bias_fn=B_OP22; c22.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 128, A, W, c22, POOL_ID, nullptr); W = V;\n\nConv d23; d23.K=3; d23.P=1; d23.S=2; d23.weight_fn=W_OP23; d23.bias_fn=B_OP23; d23.act=ACT_RELU;\nV = noodle_dwconv_float(A, 128, B, W, d23, POOL_ID, nullptr); W = V;\n\nConv c24; c24.K=1; c24.P=0; c24.S=1; c24.weight_fn=W_OP24; c24.bias_fn=B_OP24; c24.act=ACT_RELU;\nV = noodle_conv_float(B, 128, 256, A, W, c24, POOL_ID, nullptr); W = V;\n\nConv d25; d25.K=3; d25.P=1; d25.S=1; d25.weight_fn=W_OP25; d25.bias_fn=B_OP25; d25.act=ACT_RELU;\nV = noodle_dwconv_float(A, 256, B, W, d25, POOL_ID, nullptr); W = V;\n\nConv c26; c26.K=1; c26.P=0; c26.S=1; c26.weight_fn=W_OP26; c26.bias_fn=B_OP26; c26.act=ACT_RELU;\nV = noodle_conv_float(B, 256, 256, A, W, c26, POOL_ID, nullptr); W = V;\n\nnoodle_gap(A, 256, W);\n\nfloat out2[2];\nFCNFile fcf; fcf.weight_fn = FC_W; fcf.bias_fn = FC_B; fcf.act = ACT_SOFTMAX;\n(void)noodle_fcn((const float*)A, 256, 2, out2, fcf, nullptr);\n\n</code></pre>"},{"location":"vww-esp32/#testing-scenario","title":"Testing Scenario","text":"<ol> <li>Image Processing: A Python script loads an image, converts it into an array, and sends it to the ESP32 via serial communication.</li> <li>Inference: The ESP32 receives the data, processes the layers, and calculates the prediction.</li> <li>Output: The ESP32 returns the classification results and the total execution time to the host.</li> </ol> <pre><code>Found 10 images in 'non_person'\nPY DBG first pixel normalized: R=0.094118 G=0.086275 B=0.105882\n[1/10] COCO_train2014_000000387393.jpg -&gt; P0=0.964005 P1=0.035995 pred=0 time_ms=29869\n[2/10] COCO_train2014_000000395007.jpg -&gt; P0=0.926328 P1=0.073672 pred=0 time_ms=29870\n[3/10] COCO_val2014_000000556005.jpg -&gt; P0=0.850917 P1=0.149083 pred=0 time_ms=29870\n[4/10] COCO_train2014_000000091234.jpg -&gt; P0=0.931560 P1=0.068440 pred=0 time_ms=29869\n[5/10] COCO_train2014_000000257732.jpg -&gt; P0=0.916183 P1=0.083817 pred=0 time_ms=29869\n[6/10] COCO_train2014_000000187844.jpg -&gt; P0=0.668761 P1=0.331239 pred=0 time_ms=29870\n[7/10] COCO_train2014_000000037880.jpg -&gt; P0=0.960637 P1=0.039363 pred=0 time_ms=29869\n[8/10] COCO_val2014_000000326854.jpg -&gt; P0=0.983913 P1=0.016087 pred=0 time_ms=29870\n[9/10] COCO_train2014_000000044842.jpg -&gt; P0=0.973420 P1=0.026580 pred=0 time_ms=29871\n[10/10] COCO_val2014_000000558362.jpg -&gt; P0=0.774368 P1=0.225632 pred=0 time_ms=29870\n\nSummary: n=10 avg_P1=0.104991 avg_time_ms=29869.7\n\nFound 10 images in 'person'\nPY DBG first pixel normalized: R=0.819608 G=0.403922 B=0.301961\n[1/10] COCO_val2014_000000533123.jpg -&gt; P0=0.273155 P1=0.726845 pred=1 time_ms=29870\n[2/10] COCO_val2014_000000127516.jpg -&gt; P0=0.355667 P1=0.644333 pred=1 time_ms=29870\n[3/10] COCO_val2014_000000287741.jpg -&gt; P0=0.862113 P1=0.137887 pred=0 time_ms=29870\n[4/10] COCO_train2014_000000523490.jpg -&gt; P0=0.548813 P1=0.451187 pred=0 time_ms=29870\n[5/10] COCO_train2014_000000091056.jpg -&gt; P0=0.687100 P1=0.312900 pred=0 time_ms=29870\n[6/10] COCO_train2014_000000101218.jpg -&gt; P0=0.336044 P1=0.663956 pred=1 time_ms=29871\n[7/10] COCO_train2014_000000169510.jpg -&gt; P0=0.844256 P1=0.155744 pred=0 time_ms=29870\n[8/10] COCO_val2014_000000138995.jpg -&gt; P0=0.662459 P1=0.337541 pred=0 time_ms=29870\n[9/10] COCO_val2014_000000022793.jpg -&gt; P0=0.911852 P1=0.088148 pred=0 time_ms=29871\n[10/10] COCO_train2014_000000336552.jpg -&gt; P0=0.294411 P1=0.705589 pred=1 time_ms=29870\n\nSummary: n=10 avg_P1=0.422413 avg_time_ms=29870.2\n</code></pre> <ol> <li>Validation: Compare the results with Python and Tensorflow using the provided trained <code>.tflite</code>.5. </li> </ol> <pre><code>[1/10] COCO_train2014_000000387393.jpg -&gt; P0=0.974202 P1=0.025798 pred=0\n[2/10] COCO_train2014_000000395007.jpg -&gt; P0=0.790333 P1=0.209667 pred=0\n[3/10] COCO_val2014_000000556005.jpg -&gt; P0=0.942274 P1=0.057726 pred=0\n[4/10] COCO_train2014_000000091234.jpg -&gt; P0=0.797478 P1=0.202522 pred=0\n[5/10] COCO_train2014_000000257732.jpg -&gt; P0=0.879772 P1=0.120228 pred=0\n[6/10] COCO_train2014_000000187844.jpg -&gt; P0=0.777542 P1=0.222458 pred=0\n[7/10] COCO_train2014_000000037880.jpg -&gt; P0=0.977813 P1=0.022187 pred=0\n[8/10] COCO_val2014_000000326854.jpg -&gt; P0=0.969205 P1=0.030795 pred=0\n[9/10] COCO_train2014_000000044842.jpg -&gt; P0=0.974536 P1=0.025464 pred=0\n[10/10] COCO_val2014_000000558362.jpg -&gt; P0=0.896006 P1=0.103994 pred=0\n\nSummary: n=10 avg_P1=0.102084\n\n[1/10] COCO_val2014_000000533123.jpg -&gt; P0=0.001624 P1=0.998376 pred=1\n[2/10] COCO_val2014_000000127516.jpg -&gt; P0=0.068915 P1=0.931085 pred=1\n[3/10] COCO_val2014_000000287741.jpg -&gt; P0=0.520537 P1=0.479463 pred=0\n[4/10] COCO_train2014_000000523490.jpg -&gt; P0=0.063276 P1=0.936723 pred=1\n[5/10] COCO_train2014_000000091056.jpg -&gt; P0=0.072531 P1=0.927469 pred=1\n[6/10] COCO_train2014_000000101218.jpg -&gt; P0=0.053075 P1=0.946925 pred=1\n[7/10] COCO_train2014_000000169510.jpg -&gt; P0=0.385355 P1=0.614645 pred=1\n[8/10] COCO_val2014_000000138995.jpg -&gt; P0=0.243333 P1=0.756667 pred=1\n[9/10] COCO_val2014_000000022793.jpg -&gt; P0=0.694776 P1=0.305224 pred=0\n[10/10] COCO_train2014_000000336552.jpg -&gt; P0=0.005157 P1=0.994843 pred=1\n\nSummary: n=10 avg_P1=0.789142\n</code></pre>"},{"location":"vww-esp32/#remarks","title":"Remarks","text":"<p>Discrepancies currently exist between TensorFlow and Noodle outputs. Since reusing pre-trained weights and biases requires bit-perfect process parity, we are finding it difficult to trace and align specific execution steps. To ensure consistency moving forward, it may be more efficient to train models from scratch within our own ecosystem and develop a custom model exporter.</p>"}]}