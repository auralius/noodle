{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Noodle is a lightweight convolutional neural network inference library designed for microcontroller units with very limited RAM.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>API Reference Documentation</li> <li>Anomaly Detection <ul> <li>A fully connected autoencoder used in the MLPerf Tiny anomaly detection benchmark</li> <li>ESP32-S3 Platform</li> </ul> </li> <li>Single Digit Recognition with Touch-Based Interface<ul> <li>LeNet 5 implementation on ESP32</li> <li>Two layer fully connected network on ATmega328 (Arduino Uno R3)</li> </ul> </li> </ul>"},{"location":"#persistent-identifier","title":"Persistent identifier","text":""},{"location":"#authors","title":"Authors","text":"<ul> <li>Auralius Manurung \u2014 Universitas Telkom, Bandung (auralius.manurung@ieee.org)</li> <li>Lisa Kristiana \u2014 ITENAS, Bandung (lisa@itenas.ac.id)</li> </ul>"},{"location":"ad-esp32/","title":"Anomaly Detection on ESP32","text":""},{"location":"ad-esp32/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>Download the whole project files here (Visual Code and PlatformIO).</li> <li>Google Colab link. This is used to extract stored weights and biases from <code>ad01_fp32.tflite</code>.</li> <li>Weights, biases and test datasets. These are extracted from the provided <code>ad01_fp32.tflite</code>.</li> </ul> </li> </ul>"},{"location":"ad-esp32/#mlperf-tiny-anomaly-detection-model","title":"MLPerf Tiny anomaly detection model","text":"<p>In this section, we will implement the anomali detection which is one of benchmark found in MLPerf Tiny. The network is entirely made of dense layers with ReLU activations (Auto-encoder Fully-Connected or AEFC). </p> Layer # Type Input dim Output dim Activation Extracted weights and biases 1 Dense 640 128 ReLU <code>w01.txt</code>, <code>w02.txt</code> 2 Dense 128 128 ReLU <code>w03.txt</code>, <code>w04.txt</code> 3 Dense 128 128 ReLU <code>w05.txt</code>, <code>w06.txt</code> 4 Dense 128 128 ReLU <code>w07.txt</code>, <code>w08.txt</code> 5 Dense 128 8 ReLU <code>w09.txt</code>, <code>w10.txt</code> 6 Dense 8 128 ReLU <code>w11.txt</code>, <code>w12.txt</code> 7 Dense 128 128 ReLU <code>w13.txt</code>, <code>w14.txt</code> 8 Dense 128 128 ReLU <code>w15.txt</code>, <code>w16.txt</code> 9 Dense 128 128 ReLU <code>w17.txt</code>, <code>w18.txt</code> 10 Dense 128 640 None (Linear) <code>w19.txt</code>, <code>w20.txt</code>"},{"location":"ad-esp32/#extracting-the-weights-and-biases","title":"Extracting the weights and biases","text":"<p>The repository provides weights and biases that we can use directly. For this purpose, we selected the trained network stored in <code>ad01_fp32.tflite</code>.</p>"},{"location":"ad-esp32/#generating-txt-from-wav-files","title":"Generating <code>.txt</code> from <code>.wav</code> files","text":"<p>For our benchmark test, we perform the same preprocessing pipeline used during training (as found in this part of the repository). However, we will run the test offline and with viewer data. The ESP32 never processes raw audio. It only consumes precomputed feature vectors stored as text files. Each input <code>.wav</code> file is converted into multiple fixed-length feature vectors using the following steps:</p> <ul> <li>Load WAV file: the audio file is loaded using its native sampling rate (no resampling). The WAV file is 11 seconds with 342 frames.</li> <li>Feature extraction: a log-mel spectrogram is computed using the same parameters defined in <code>baseline.yaml</code>:<ul> <li><code>n_mels = 128</code></li> <li><code>frames = 5</code></li> <li><code>n_fft = 1024</code></li> <li><code>hop_length = 512</code></li> <li><code>power = 2.0</code></li> </ul> </li> <li>Temporal cropping: only the central portion of the spectrogram is kept: <code>frames 50 to 250 \u2192 200</code> frames total.</li> <li>Sliding window segmentation: a sliding window of length frames = 5 is applied across the cropped spectrogram, producing: <code>200 \u2212 5 + 1 = 196</code> feature vectors per WAV file.</li> <li>Flattening and storage: each window is flattened into a 1-D vector of size: <code>inputDim = n_mels \u00d7 frames = 128 \u00d7 5 = 640</code> and stored as a <code>float32</code> text file:</li> </ul> <pre><code>&lt;wav_name&gt;_part000.txt\n&lt;wav_name&gt;_part001.txt\n\u2026\n&lt;wav_name&gt;_part195.txt\n</code></pre> <ul> <li>Take 5 parts from the an anomaly set and name them <code>anom1.txt</code> to <code>anom5.txt</code>. </li> <li>Take 5 parts from the a normal set and name them <code>norm1.txt</code> to <code>norm5.txt</code>. </li> </ul> <p>These <code>.txt</code> files represent the actual inputs to the auto-encoder and match exactly the data format used during training and evaluation in the original baseline implementation.</p>"},{"location":"ad-esp32/#esp32-inference-workflow","title":"ESP32 inference workflow","text":"<p>On the ESP32, we will perform the following inference procedures:</p> <ul> <li>For a given WAV sample, all corresponding <code>*_partXXX.bin</code> files are loaded sequentially from SD card (or FFAT).</li> <li>Each .bin file is read into a <code>float[640]</code> input buffer.</li> <li>The input vector is passed through the auto-encoder implemented using Noodle, producing a reconstructed output vector of the same size.</li> <li>The mean squared reconstruction error (MSE) between input and output is computed for that window.</li> <li>Errors are accumulated across all 196 windows.</li> <li>The final anomaly score for the WAV file is computed as the average reconstruction error: <code>score = mean(MSE_part_0 \u2026 MSE_part_195)</code></li> </ul> <p>We will only retain the final scalar score and discard the individually reconstructed vectors immediately to minimize memory usage.</p>"},{"location":"ad-esp32/#hardware","title":"Hardware","text":"<p>For this benchmark, we will use ESP32-S3-N16R8 which gives us plenty of space in the flash.</p>"},{"location":"ad-esp32/#testing-scenario","title":"Testing scenario","text":"<ul> <li>Apply 5 parts from an anomaly dataset \u279c 5/196 of a full WAV.</li> <li>Apply 5 parts from a normal dataset \u279c 5/196 of a full WAV.</li> <li>Each part is <code>float32[640]</code> values (2560 bytes).</li> <li>ESP32 returns <code>mse</code> and elapsed time (<code>us</code>) for each part.</li> </ul>"},{"location":"ad-esp32/#code-on-the-esp-side","title":"Code on the ESP side","text":"<pre><code>static constexpr uint16_t INPUT_DIM = 640;\nstatic constexpr uint16_t HIDDEN_DIM = 128;\nstatic constexpr uint16_t BOTTLENECK_DIM = 8;\n\n// Ping-pong buffers\nstatic float BUF1[INPUT_DIM];\nstatic float BUF2[INPUT_DIM];\n\n// Copy of input for MSE\nstatic float X0[INPUT_DIM];\n\nFCNFile L1;  \nL1.weight_fn  = \"/w01.txt\"; L1.bias_fn  = \"/w02.txt\"; L1.act  = ACT_RELU;\n\nFCNFile L2;  \nL2.weight_fn  = \"/w03.txt\"; L2.bias_fn  = \"/w04.txt\"; L2.act  = ACT_RELU;\n\nFCNFile L3;  \nL3.weight_fn  = \"/w05.txt\"; L3.bias_fn  = \"/w06.txt\"; L3.act  = ACT_RELU;\n\nFCNFile L4;  \nL4.weight_fn  = \"/w07.txt\"; L4.bias_fn  = \"/w08.txt\"; L4.act  = ACT_RELU;\n\nFCNFile L5;  \nL5.weight_fn  = \"/w09.txt\"; L5.bias_fn  = \"/w10.txt\"; L5.act  = ACT_RELU;\n\nFCNFile L6;  \nL6.weight_fn  = \"/w11.txt\"; L6.bias_fn  = \"/w12.txt\"; L6.act  = ACT_RELU;\n\nFCNFile L7;  \nL7.weight_fn  = \"/w13.txt\"; L7.bias_fn  = \"/w14.txt\"; L7.act  = ACT_RELU;\n\nFCNFile L8;  \nL8.weight_fn  = \"/w15.txt\"; L8.bias_fn  = \"/w16.txt\"; L8.act  = ACT_RELU;\n\nFCNFile L9;  \nL9.weight_fn  = \"/w17.txt\"; L9.bias_fn  = \"/w18.txt\"; L9.act  = ACT_RELU;\n\nFCNFile L10; \nL10.weight_fn = \"/w19.txt\"; L10.bias_fn = \"/w20.txt\"; L10.act = ACT_NONE;\n\nuint16_t V = INPUT_DIM;\n\n// 640 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 8 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 128 -&gt; 640\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L1,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L2,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L3,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L4,  NULL);\nV = noodle_fcn(BUF1, V, BOTTLENECK_DIM,  BUF2, L5,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L6,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L7,  NULL);\nV = noodle_fcn(BUF2, V, HIDDEN_DIM,      BUF1, L8,  NULL);\nV = noodle_fcn(BUF1, V, HIDDEN_DIM,      BUF2, L9,  NULL);\nV = noodle_fcn(BUF2, V, INPUT_DIM,       BUF1, L10, NULL);\n</code></pre>"},{"location":"ad-esp32/#compare-esp32-and-google-colab","title":"Compare ESP32 and Google Colab","text":"<p>ESP 32</p> <pre><code>=== anom set ===\nanom1: mse=9.17803478 us=23812318\nanom2: mse=8.465662 us=23812164\nanom3: mse=8.66184902 us=23812145\nanom4: mse=9.07823086 us=23812145\nanom5: mse=8.99418163 us=23812144\nMean anom MSE = 8.87559128\n\n=== norm set ===\nnorm1: mse=10.8290482 us=23812112\nnorm2: mse=11.2695885 us=23812183\nnorm3: mse=11.530509 us=23812167\nnorm4: mse=11.6987247 us=23812152\nnorm5: mse=11.0928583 us=23812148\nMean norm MSE = 11.2841454\n\nDONE (processed anom1..5 + norm1..5)\n</code></pre> <p>Google Colab</p> <ul> <li>Google Colab link</li> <li>Weights, biases and test datasets</li> </ul> <pre><code>Input : input_1 shape= [  1 640] dtype= &lt;class 'numpy.float32'&gt;\nOutput: Identity shape= [  1 640] dtype= &lt;class 'numpy.float32'&gt;\n\n--- ANOM ---\n[TFLite] /content/sample_data/anom1.txt: mse=9.17908482\n[TFLite] /content/sample_data/anom2.txt: mse=8.46575602\n[TFLite] /content/sample_data/anom3.txt: mse=8.6624254\n[TFLite] /content/sample_data/anom4.txt: mse=9.07872793\n[TFLite] /content/sample_data/anom5.txt: mse=8.99621678\n\n--- NORM ---\n[TFLite] /content/sample_data/norm1.txt: mse=10.82921\n[TFLite] /content/sample_data/norm2.txt: mse=11.2699071\n[TFLite] /content/sample_data/norm3.txt: mse=11.5304345\n[TFLite] /content/sample_data/norm4.txt: mse=11.6980121\n[TFLite] /content/sample_data/norm5.txt: mse=11.0927933\n\n--- SUMMARY ---\nnorm mean=11.2840714 std=0.344865398 min=10.82921 max=11.6980121\nanom mean=8.87644219 std=0.300551306 min=8.46575602 max=9.17908482\nanom/norm mean ratio = 0.786634706\n\nDone.\n</code></pre>"},{"location":"ad-esp32/#inference-parity-esp32-vs-colab","title":"Inference parity: ESP32 vs Colab","text":""},{"location":"ad-esp32/#anomaly-set","title":"Anomaly set","text":"Sample ESP32 MSE TFLite MSE \u0394 <code>anom1.txt</code> 9.1780 9.1791 ~0.001 <code>anom2.txt</code> 8.4657 8.4658 ~0.0001 <code>anom3.txt</code> 8.6618 8.6624 ~0.0006 <code>anom4.txt</code> 9.0782 9.0787 ~0.0005 <code>anom5.txt</code> 8.9942 8.9962 ~0.002"},{"location":"ad-esp32/#normal-set","title":"Normal set","text":"Sample ESP32 MSE TFLite MSE \u0394 (abs) <code>norm1.txt</code> 10.82897 10.82921 \u2248 0.00024 <code>norm2.txt</code> 11.27035 11.26991 \u2248 0.00044 <code>norm3.txt</code> 11.53070 11.53043 \u2248 0.00027 <code>norm4.txt</code> 11.69870 11.69801 \u2248 0.00069 <code>norm5.txt</code> 11.09396 11.09279 \u2248 0.00117"},{"location":"api/","title":"Brief API References","text":"<p>Tiny, file-streamed CNN/ML primitives for microcontrollers with very small RAM budgets.  </p> <p>Noodle supports in-memory and file-system-streamed (SD/FFat/SD_MMC/LittleFS/SdFat) execution for:</p> <ul> <li>2D/1D convolution (+ optional pooling)</li> <li>bias + activation</li> <li>flatten</li> <li>fully connected (FCN)</li> <li>softmax / sigmoid</li> </ul> <p>This reference is generated from <code>noodle.h</code>, <code>noodle_config.h</code>, and <code>noodle_fs.h</code>.</p>"},{"location":"api/#1-configuration","title":"1) Configuration","text":""},{"location":"api/#11-filesystem-backend-selection-noodle_configh-noodle_fsh","title":"1.1 Filesystem backend selection (<code>noodle_config.h</code> / <code>noodle_fs.h</code>)","text":"<p>Define exactly one before including <code>noodle.h</code>:</p> <ul> <li><code>NOODLE_USE_SDFAT</code></li> <li><code>NOODLE_USE_SD_MMC</code></li> <li><code>NOODLE_USE_FFAT</code></li> <li><code>NOODLE_USE_LITTLEFS</code></li> </ul> <p><code>noodle_fs.h</code> exposes:</p> <ul> <li><code>using NDL_File = ...</code> </li> <li><code>FsFile</code> for SdFat, otherwise Arduino <code>File</code></li> <li><code>NOODLE_FS</code> handle  </li> <li><code>SdFat NOODLE_FS</code> (extern, defined in <code>noodle.cpp</code>) or the FS singleton (<code>FFat</code>, <code>SD_MMC</code>, <code>LittleFS</code>)</li> <li>Backend-neutral open/remove helpers:</li> <li><code>NDL_File noodle_fs_open_read(const char* path)</code></li> <li><code>NDL_File noodle_fs_open_write(const char* path)</code></li> <li><code>bool noodle_fs_remove(const char* path)</code></li> </ul>"},{"location":"api/#12-pooling-mode-noodle_configh","title":"1.2 Pooling mode (<code>noodle_config.h</code>)","text":"<p>Compile-time pooling mode for 2D pooling:</p> <ul> <li><code>#define NOODLE_POOL_MAX  1</code></li> <li><code>#define NOODLE_POOL_MEAN 2</code></li> <li><code>#define NOODLE_POOL_MODE NOODLE_POOL_MAX</code> or <code>NOODLE_POOL_MEAN</code></li> </ul>"},{"location":"api/#2-naming-convention-for-streamed-channel-files-two-letter-indices","title":"2) Naming convention for streamed channel files (two-letter indices)","text":""},{"location":"api/#convolution-kernels-4d-tensors","title":"Convolution kernels (4D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;&lt;in&gt;&lt;out&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> <li><code>&lt;in&gt;</code> is a two-letter code for the input-channel index (<code>aa</code>, <code>ab</code>, \u2026, <code>zz</code>)</li> <li><code>&lt;out&gt;</code> is a two-letter code for the output-channel index</li> </ul> <p>Example: <code>w01aaab.txt</code></p> <p>Each file contains one flattened spatial kernel for a single <code>(input channel, output channel)</code> pair.</p>"},{"location":"api/#dense-weights-2d-tensors","title":"Dense weights (2D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> </ul> <p>Example: <code>w03.txt</code></p> <p>Dense weights are exported as a transposed, flattened array, one value per line.</p>"},{"location":"api/#bias-vectors-1d-tensors","title":"Bias vectors (1D tensors)","text":"<p>Filename format: <code>b&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a bias vector</li> </ul> <p>Example: <code>b02.txt</code></p> <p>Each file contains flattened bias values, one value per line.</p>"},{"location":"api/#3-types","title":"3) Types","text":""},{"location":"api/#31-activation","title":"3.1 Activation","text":"<pre><code>enum Activation : uint8_t { ACT_NONE = 0, ACT_RELU = 1, ACT_SOFTMAX = 2 };\n</code></pre>"},{"location":"api/#32-conv-shared-params-for-1d2d","title":"3.2 <code>Conv</code> (shared params for 1D/2D)","text":"<pre><code>struct Conv {\n  uint16_t K;              // kernel size (2D: KxK, 1D: K)\n  uint16_t P = 0;          // padding\n  uint16_t S = 1;          // stride\n  const char *weight_fn;   // file template for weights\n  const char *bias_fn;     // bias filename\n  Activation act = ACT_RELU;\n};\n</code></pre>"},{"location":"api/#33-pool","title":"3.3 <code>Pool</code>","text":"<pre><code>struct Pool {\n  uint16_t M = 2;  // pool kernel\n  uint16_t T = 2;  // pool stride\n};\n</code></pre>"},{"location":"api/#34-progress-callback","title":"3.4 Progress callback","text":"<pre><code>typedef void (*CBFPtr)(float progress);  // progress in [0,1]\n</code></pre>"},{"location":"api/#35-fcn-parameter-blocks","title":"3.5 FCN parameter blocks","text":"<pre><code>struct FCN     \n{ \n  const char *weight_fn; \n  const char *bias_fn; \n  Activation act = ACT_RELU; \n};\n\nstruct FCNFile \n{ \n  const char *weight_fn; \n  const char *bias_fn; \n  Activation act = ACT_RELU; \n};\n\nstruct FCNMem {\n  const float *weight;  // row-major [n_outputs, n_inputs]\n  const float *bias;    // length n_outputs\n  Activation act = ACT_RELU;\n};\n</code></pre>"},{"location":"api/#4-initialization-scratch-buffers","title":"4) Initialization &amp; scratch buffers","text":""},{"location":"api/#41-temporary-buffers-required-for-some-streamed-ops","title":"4.1 Temporary buffers (required for some streamed ops)","text":"<pre><code>void noodle_setup_temp_buffers(void *b1, void *b2 = NULL);\n</code></pre> <p>Provides two reusable scratch buffers used internally by file-streaming convolution/FCN variants.</p> <p>Typical guidance (from header docs): - temp buffer #1: \u2265 <code>W*W*sizeof(input_element)</code> - temp buffer #2: \u2265 <code>W*W*sizeof(float)</code> (accumulator)</p>"},{"location":"api/#5-filesystem-utilities","title":"5) Filesystem utilities","text":""},{"location":"api/#51-open-for-write-delete-first","title":"5.1 Open for write (delete first)","text":"<pre><code>NDL_File noodle_open_file_for_write(const char* fn);\n</code></pre>"},{"location":"api/#52-read-bytes-until-terminator","title":"5.2 Read bytes until terminator","text":"<pre><code>size_t noodle_read_bytes_until(NDL_File &amp;file, char terminator, char *buffer, size_t length);\n</code></pre> <p>Reads into <code>buffer</code> (always NUL-terminated), stops at <code>terminator</code> or <code>length-1</code>.</p>"},{"location":"api/#53-backend-init","title":"5.3 Backend init","text":"<pre><code>bool noodle_sd_init(int clk_pin, int cmd_pin, int d0_pin);\nbool noodle_sd_init();\n</code></pre> <p>Pins variant is meaningful for SD_MMC; default form uses backend defaults.</p>"},{"location":"api/#54-delete-file","title":"5.4 Delete file","text":"<pre><code>void noodle_delete_file(const char *fn);\n</code></pre>"},{"location":"api/#6-scalar-io-helpers-text-files","title":"6) Scalar I/O helpers (text files)","text":"<pre><code>void  noodle_write_float(NDL_File &amp;f, float d);\nfloat noodle_read_float(NDL_File &amp;f);\n\nbyte  noodle_read_byte(NDL_File &amp;f);\nvoid  noodle_write_byte(NDL_File &amp;f, byte d);\n</code></pre>"},{"location":"api/#7-memory-helpers","title":"7) Memory helpers","text":"<pre><code>float* noodle_create_buffer(uint16_t size); \nvoid   noodle_delete_buffer(float *buffer);\nvoid   noodle_reset_buffer(float *buffer, uint16_t n); \n</code></pre>"},{"location":"api/#8-array-grid-io-text-files","title":"8) Array / grid I/O (text files)","text":""},{"location":"api/#81-write","title":"8.1 Write","text":"<pre><code>void noodle_array_to_file(float *array, const char *fn, uint16_t n);\nvoid noodle_grid_to_file(byte  *grid,  const char *fn, uint16_t n); /\nvoid noodle_grid_to_file(float *grid,  const char *fn, uint16_t n);\n</code></pre>"},{"location":"api/#82-read","title":"8.2 Read","text":"<pre><code>void noodle_array_from_file(const char *fn, float *buffer, uint16_t K);\n\nvoid noodle_grid_from_file(const char *fn, byte   *buffer, uint16_t K);\nvoid noodle_grid_from_file(const char *fn, int8_t *buffer, uint16_t K);\nvoid noodle_grid_from_file(const char *fn, float  *buffer, uint16_t K);\n</code></pre>"},{"location":"api/#83-padded-accessors","title":"8.3 Padded accessors","text":"<pre><code>float noodle_get_padded_x(byte  *grid, int16_t i, int16_t j, int16_t W, int16_t P);\nfloat noodle_get_padded_x(float *grid, int16_t i, int16_t j, int16_t W, int16_t P);\n</code></pre> <p>Returns 0 outside bounds, otherwise grid value as float.</p>"},{"location":"api/#9-convolution-pooling-primitives","title":"9) Convolution &amp; pooling primitives","text":""},{"location":"api/#91-2d-convolution-in-memory-primitive","title":"9.1 2D convolution (in-memory primitive)","text":"<p>Output spatial size: <code>V = (W - K + 2P)/S + 1</code></p> <pre><code>uint16_t noodle_do_conv(byte  *grid, float *kernel, uint16_t K, uint16_t W,\n                        float *output, uint16_t P, uint16_t S);\n\nuint16_t noodle_do_conv(float *grid, float *kernel, uint16_t K, uint16_t W,\n                        float *output, uint16_t P, uint16_t S);\n</code></pre>"},{"location":"api/#92-bias-optional-activation","title":"9.2 Bias (+ optional activation)","text":"<pre><code>uint16_t noodle_do_bias(float *output, float bias, uint16_t n);  // legacy: bias + ReLU\n\nuint16_t noodle_do_bias_act(float *output, float bias, uint16_t n, Activation act);\n</code></pre>"},{"location":"api/#93-2d-pooling-mode-selected-by-noodle_pool_mode","title":"9.3 2D pooling (mode selected by <code>NOODLE_POOL_MODE</code>)","text":"<p>Output spatial size: <code>V_out = (V - K)/S + 1</code></p> <pre><code>uint16_t noodle_do_pooling(float *input, uint16_t W, uint16_t K, uint16_t S, const char *fn);\nuint16_t noodle_do_pooling(const float *input, uint16_t W, uint16_t K, uint16_t S, float *output);\n</code></pre>"},{"location":"api/#94-1d-pooling-file-output","title":"9.4 1D pooling (file output)","text":"<pre><code>uint16_t noodle_do_pooling1d(float *input, uint16_t W, uint16_t K, uint16_t S, const char *fn);\n</code></pre>"},{"location":"api/#95-1d-convolution-primitive","title":"9.5 1D convolution primitive","text":"<p>Output length: <code>V = (W - K + 2P)/S + 1</code></p> <pre><code>uint16_t noodle_do_conv1d(float *input, float *kernel, uint16_t W, uint16_t K,\n                          float *output, uint16_t P, uint16_t S);\n</code></pre>"},{"location":"api/#10-streamed-convolution-apis","title":"10) Streamed Convolution APIs","text":"<p>These functions use the filename tokenization convention (two-letter indices) where applicable.</p>"},{"location":"api/#101-2d-convolution-optional-pooling","title":"10.1 2D convolution (+ optional pooling)","text":"<p>File \u2192 File (BYTE input feature maps)</p> <pre><code>uint16_t noodle_conv_byte(const char *in_fn,\n                          uint16_t n_inputs,\n                          uint16_t n_outputs,\n                          const char *out_fn,\n                          uint16_t W,\n                          const Conv &amp;conv,\n                          const Pool &amp;pool,\n                          CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 File (FLOAT input feature maps)</p> <pre><code>uint16_t noodle_conv_float(const char *in_fn,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           const char *out_fn,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 Memory (FLOAT inputs, output tensor <code>[O, Wo, Wo]</code>)</p> <pre><code>uint16_t noodle_conv_float(const char *in_fn,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           float *output,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Memory \u2192 File (FLOAT inputs)</p> <pre><code>uint16_t noodle_conv_float(float *input,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           const char *out_fn,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Memory \u2192 Memory (FLOAT inputs)</p> <pre><code>uint16_t noodle_conv_float(float *input,\n                           uint16_t n_inputs,\n                           uint16_t n_outputs,\n                           float *output,\n                           uint16_t W,\n                           const Conv &amp;conv,\n                           const Pool &amp;pool,\n                           CBFPtr progress_cb = NULL);\n</code></pre> <p>Return value is the output spatial size after pooling (if enabled by your <code>Pool</code> settings).</p>"},{"location":"api/#102-1d-convolution-streamed-tokenized-filenames","title":"10.2 1D convolution (streamed, tokenized filenames)","text":"<p>File \u2192 File, with pooling</p> <pre><code>uint16_t noodle_conv1d(float *input, float *output,\n                       uint16_t n_inputs, uint16_t n_outputs,\n                       const char *in_fn, const char *out_fn,\n                       uint16_t W, const Conv &amp;conv, const Pool &amp;pool,\n                       CBFPtr progress_cb = NULL);\n</code></pre> <p>File \u2192 File, no pooling</p> <pre><code>uint16_t noodle_conv1d(float *input, float *output,\n                       uint16_t n_inputs, uint16_t n_outputs,\n                       const char *in_fn, const char *out_fn,\n                       uint16_t W, const Conv &amp;conv,\n                       CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#11-fully-connected-fcn","title":"11) Fully Connected (FCN)","text":"<p>All FCN variants compute: - <code>y = W\u00b7x + b</code> - then apply activation (<code>ACT_NONE</code> or <code>ACT_RELU</code> in typical usage; <code>ACT_SOFTMAX</code> exists but softmax is usually applied separately)</p>"},{"location":"api/#111-memory-int8-file-params-from-files","title":"11.1 Memory (int8) \u2192 File (params from files)","text":"<pre><code>uint16_t noodle_fcn(const int8_t *input, uint16_t n_inputs, uint16_t n_outputs,\n                    const char *out_fn, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#112-file-float-text-file-params-from-files","title":"11.2 File (float text) \u2192 File (params from files)","text":"<pre><code>uint16_t noodle_fcn(const char *in_fn, uint16_t n_inputs, uint16_t n_outputs,\n                    const char *out_fn, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#113-memory-float-memory-in-memory-params","title":"11.3 Memory (float) \u2192 Memory (in-memory params)","text":"<pre><code>uint16_t noodle_fcn(const float *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNMem &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#114-memory-byte-memory-params-from-files","title":"11.4 Memory (byte) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const byte *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#115-memory-int8-memory-params-from-files","title":"11.5 Memory (int8) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const int8_t *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#116-file-float-text-memory-params-from-files","title":"11.6 File (float text) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const char *in_fn, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb = NULL);\n</code></pre>"},{"location":"api/#117-memory-float-memory-params-from-files","title":"11.7 Memory (float) \u2192 Memory (params from files)","text":"<pre><code>uint16_t noodle_fcn(const float *input, uint16_t n_inputs, uint16_t n_outputs,\n                    float *output, const FCNFile &amp;fcn,\n                    CBFPtr progress_cb);\n</code></pre>"},{"location":"api/#12-flatten","title":"12) Flatten","text":""},{"location":"api/#121-file-memory-flatten","title":"12.1 File \u2192 Memory flatten","text":"<pre><code>uint16_t noodle_flat(const char *in_fn, float *output, uint16_t V, uint16_t n_filters);\n</code></pre> <p>Reads <code>n_filters</code> feature maps from files named by <code>in_fn</code> template and writes a vector of length <code>V*V*n_filters</code>.</p>"},{"location":"api/#122-memory-memory-flatten","title":"12.2 Memory \u2192 Memory flatten","text":"<pre><code>uint16_t noodle_flat(float *input, float *output, uint16_t V, uint16_t n_filters);\n</code></pre>"},{"location":"api/#13-activations","title":"13) Activations","text":"<pre><code>uint16_t noodle_soft_max(float *input_output, uint16_t n);\nuint16_t noodle_sigmoid(float *input_output, uint16_t n);\n</code></pre>"},{"location":"api/#14-misc-utilities","title":"14) Misc utilities","text":""},{"location":"api/#141-tensor-slicing-helper","title":"14.1 Tensor slicing helper","text":"<pre><code>static inline float* noodle_slice(float* flat, size_t W, size_t z);\n</code></pre> <p>Slices a stacked <code>[Z, W, W]</code> tensor stored as contiguous planes.</p>"},{"location":"api/#142-read-first-line-of-a-text-file","title":"14.2 Read first line of a text file","text":"<pre><code>void noodle_read_top_line(const char* fn, char *line, size_t maxlen);\n</code></pre>"},{"location":"api/#notes","title":"Notes","text":"<ul> <li>Text file format: Tensor/weight/bias I/O uses human-readable text, \u201cone float per line\u201d.</li> <li>Filename templates are mutated in-place: pass writable <code>char[]</code> buffers.</li> </ul>"},{"location":"lenet-5-esp32/","title":"LeNet-5 on ESP32","text":""},{"location":"lenet-5-esp32/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>Download the whole project files here (Visual Code and PlatformIO).</li> <li>Google Colab notebook </li> <li>Dataset link</li> </ul> </li> <li>Video demonstration :</li> </ul>"},{"location":"lenet-5-esp32/#plan","title":"Plan","text":"<p>We aim to implement a slightly modified LeNet-5 network that runs entirely on-device on an ESP32. To reflect a realistic inference scenario, user input will be captured via a touchscreen interface. This means the project involves not only the neural network itself, but also a preprocessing pipeline that converts raw touch input into a digitized 28\u00d728 representation suitable for inference.</p> <p>In short, the system consists of:</p> <ul> <li>on-device CNN inference</li> <li>touchscreen-based data acquisition</li> <li>preprocessing and normalization tailored to embedded constraints</li> </ul> <p>In this documentation, we skip the preprocessing pipeline and focus on the neural network implementation using the NOODLE framework.</p> <p>Implementation steps:</p> <ol> <li>Design the network and train it (e.g., in Google Colab).</li> <li>Export weights and biases.</li> <li>Decide where each parameter lives:<ul> <li>File storage: SD card (SPI/SDIO), or flash filesystem (FFat/LittleFS)</li> <li>In-memory storage: <code>const</code> in flash, or SRAM/PSRAM</li> </ul> </li> <li>Implement the network layer-by-layer using NOODLE</li> </ol>"},{"location":"lenet-5-esp32/#cheap-yellow-display-cyd","title":"Cheap Yellow Display (CYD)","text":"<p>The CYD is an ESP32-based development board with an integrated LCD display and touchscreen. The display measures 2.8 inches with a 320\u00d7240 pixel resolution. Due to its low cost and decent performance, the CYD has gained popularity in the ESP32 community, with several public repositories and example projects available, such as ESP32-Cheap-Yellow-Display.</p> <p></p> <p>The CYD uses SPI for both the display and the touch controller. These are typically wired with separate chip-select lines (and on some CYD variants, separate SPI buses), which helps when tracking continuous touch input for handwriting.</p> <p>As an alternative, we can also use displays where the TFT uses a parallel bus (or SPI) while the touch uses analog inputs, such as MCUFRIEND_kbv UNO shields.</p>"},{"location":"lenet-5-esp32/#original-architecture-1998","title":"Original architecture (1998)","text":"<p>The following figure and table describe the original LeNet-5 architecture.</p> <p></p> Layer Type Kernel / Stride Input shape Output shape Notes C1 Convolution 6\u00d75\u00d75 1\u00d732\u00d732 6\u00d728\u00d728 tanh S2 Subsampling 2\u00d72 / 2 6\u00d728\u00d728 6\u00d714\u00d714 average pooling (+ learnable scale/bias in classic LeNet) C3 Convolution 16\u00d75\u00d75 6\u00d714\u00d714 16\u00d710\u00d710 tanh, sparse connection map S4 Subsampling 2\u00d72 / 2 16\u00d710\u00d710 16\u00d75\u00d75 average pooling C5 Convolution 120\u00d75\u00d75 16\u00d75\u00d75 120\u00d71\u00d71 effectively fully connected F6 Fully connected \u2014 120 84 tanh Output Fully connected \u2014 84 10 Euclidean RBF"},{"location":"lenet-5-esp32/#slightly-modified-architecture","title":"Slightly modified architecture","text":"<p>Because the C5 (120\u00d75\u00d75) layer is equivalent to a dense layer from 400 \u2192 120, we implement it as flatten + FC:</p> Layer Type Kernel / Stride Input shape Output shape Notes C1 Convolution 6\u00d75\u00d75 1\u00d732\u00d732 6\u00d728\u00d728 ReLU S2 Subsampling 2\u00d72 / 2 6\u00d728\u00d728 6\u00d714\u00d714 average pooling C3 Convolution 16\u00d75\u00d75 6\u00d714\u00d714 16\u00d710\u00d710 ReLU S4 Subsampling 2\u00d72 / 2 16\u00d710\u00d710 16\u00d75\u00d75 average pooling F5 Flatten \u2014 16\u00d75\u00d75 400 16\u00d75\u00d75 = 400 F6 FC \u2014 400 120 ReLU F7 FC \u2014 120 84 ReLU Output FC \u2014 84 10 Softmax"},{"location":"lenet-5-esp32/#training-keras-pytorch","title":"Training (Keras / PyTorch)","text":"<p>Noodle does not provide training capability. We will perform training on a computer, extract the training results and deploy them to the ESP32. In this section, we use Google Colab (Keras and Python) for training.</p> <ul> <li>Dataset link</li> <li>Google Colab notebook </li> </ul>"},{"location":"lenet-5-esp32/#naming-convention-txt","title":"Naming convention (<code>.txt</code>)","text":"<p>The provided Keras training program come with an automatic exporter function that will export weights and biases with the following naming convention.</p>"},{"location":"lenet-5-esp32/#convolution-kernels-4d-tensors","title":"Convolution kernels (4D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;&lt;in&gt;&lt;out&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> <li><code>&lt;in&gt;</code> is a two-letter code for the input-channel index (<code>aa</code>, <code>ab</code>, \u2026, <code>zz</code>)</li> <li><code>&lt;out&gt;</code> is a two-letter code for the output-channel index</li> </ul> <p>Example: <code>w01aaab.txt</code></p> <p>Each file contains one flattened spatial kernel for a single <code>(input channel, output channel)</code> pair.</p>"},{"location":"lenet-5-esp32/#dense-weights-2d-tensors","title":"Dense weights (2D tensors)","text":"<p>Filename format: <code>w&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a weight tensor</li> </ul> <p>Example: <code>w03.txt</code></p> <p>Dense weights are exported as a transposed, flattened array, one value per line.</p>"},{"location":"lenet-5-esp32/#bias-vectors-1d-tensors","title":"Bias vectors (1D tensors)","text":"<p>Filename format: <code>b&lt;NN&gt;.txt</code></p> <p>Where:</p> <ul> <li><code>&lt;NN&gt;</code> is a two-digit, zero-padded index identifying a bias vector</li> </ul> <p>Example: <code>b02.txt</code></p> <p>Each file contains flattened bias values, one value per line.</p> <p>The exporter program will print the generated files. These are some examples for the LeNet-5.</p> <pre><code>.../lenet-5/w01aaaa.txt\n.../lenet-5/w01aaab.txt\n.../lenet-5/w01aaac.txt\n.../lenet-5/b01.txt\n\n.../lenet-5/w02aaaa.txt\n.../lenet-5/w02aaab.txt\n.../lenet-5/w02afap.txt\n.../lenet-5/b02.txt\n\n.../lenet-5/w03.txt\n.../lenet-5/b03.txt\n.../lenet-5/w04.txt\n.../lenet-5/b04.txt\n.../lenet-5/w05.txt\n.../lenet-5/b05.txt\n\n</code></pre>"},{"location":"lenet-5-esp32/#on-the-esp32-side","title":"On the ESP32 side","text":"<p>For fully connected layers, NOODLE provides two parameter structures: <code>FCNMem</code> and <code>FCNFile</code>.</p> <pre><code>/** FCN parameters (filenames; no tokenization). */\nstruct FCNFile {\n  const char *weight_fn = nullptr;\n  const char *bias_fn   = nullptr;\n  Activation act = ACT_RELU;\n};\n\n/** FCN parameters for in-memory weights/bias (row-major weights [n_outputs, n_inputs]). */\nstruct FCNMem {\n  const float *weight = nullptr;\n  const float *bias   = nullptr;\n  Activation act = ACT_RELU;\n};\n</code></pre> <p>If weights/biases are in memory, we use <code>FCNMem</code>. If they are stored as files, we use <code>FCNFile</code>.</p>"},{"location":"lenet-5-esp32/#convolution-kernel-filename-pattern","title":"Convolution kernel filename pattern (<code>####</code>)","text":"<p>For convolution layers, NOODLE loads per-(input, output) kernels from files named: <code>w&lt;KK&gt;&lt;in&gt;&lt;out&gt;.txt</code> (e.g. <code>w01aaab.txt</code>).</p> <p>In this documentation, we use the shorthand pattern:</p> <ul> <li><code>w01####.txt</code> meaning: <code>w01&lt;in&gt;&lt;out&gt;.txt</code></li> <li><code>w02####.txt</code> meaning: <code>w02&lt;in&gt;&lt;out&gt;.txt</code></li> </ul> <p>Where <code>&lt;in&gt;</code> and <code>&lt;out&gt;</code> are the two-letter channel codes produced by the exporter.</p>"},{"location":"lenet-5-esp32/#parameter-placement","title":"Parameter placement","text":"Layer Files Location In-memory (<code>const</code>) File-based C1 <code>w01####.txt</code>, <code>b01.txt</code> FFat \u2714 C3 <code>w02####.txt</code>, <code>b02.txt</code> FFat \u2714 F6 <code>w03.txt</code>, <code>b03.txt</code> flash (<code>const</code>) \u2714 F7 <code>w04.txt</code>, <code>b04.txt</code> flash (<code>const</code>) \u2714 Output <code>w05.txt</code>, <code>b05.txt</code> flash (<code>const</code>) \u2714"},{"location":"lenet-5-esp32/#layer-by-layer-implementation","title":"Layer-by-layer implementation","text":"<pre><code>Conv cnn1;\ncnn1.K = 5;\ncnn1.P = 2;\ncnn1.S = 1; // same padding\ncnn1.weight_fn = \"/w01####.txt\";\ncnn1.bias_fn   = \"/b01.txt\";\n\nConv cnn2;\ncnn2.K = 5;\ncnn2.P = 0;\ncnn2.S = 1; // valid padding\ncnn2.weight_fn = \"/w02####.txt\";\ncnn2.bias_fn   = \"/b02.txt\";\n\nPool pool;\npool.M = 2;\npool.T = 2;\n\nFCNMem fcn1;\nfcn1.weight = w03;\nfcn1.bias   = b03;\nfcn1.act    = ACT_RELU;\n\nFCNMem fcn2;\nfcn2.weight = w04;\nfcn2.bias   = b04;\nfcn2.act    = ACT_RELU;\n\nFCNMem fcn3;\nfcn3.weight = w05;\nfcn3.bias   = b05;\nfcn3.act    = ACT_SOFTMAX;\n\nuint16_t V;\n\nV = noodle_conv_float(BUFFER1, 1, 6,  BUFFER3, 28, cnn1, pool, nullptr);\nV = noodle_conv_float(BUFFER3, 6, 16, BUFFER1, V,  cnn2, pool, nullptr);\n\nV = noodle_flat(BUFFER1, BUFFER3, V, 16);\n\nV = noodle_fcn(BUFFER3, V, 120, BUFFER1, fcn1, nullptr);\nV = noodle_fcn(BUFFER1, V, 84,  BUFFER3, fcn2, nullptr);\nV = noodle_fcn(BUFFER3, V, 10,  BUFFER1, fcn3, nullptr);\n</code></pre> <p>This implementation requires three buffers:</p> <ul> <li><code>BUFFER1</code>: input buffer</li> <li><code>BUFFER2</code>: temporary/scratch buffer (set via <code>noodle_setup_temp_buffers()</code>)</li> <li><code>BUFFER3</code>: output buffer</li> </ul> <p>For the modified LeNet-5 implementation:</p> <pre><code>Input image        \u2192 BUFFER1\nConv + Pool #1     BUFFER1 \u2192 BUFFER3\nConv + Pool #2     BUFFER3 \u2192 BUFFER1\nFlatten            BUFFER1 \u2192 BUFFER3\nDense #1           BUFFER3 \u2192 BUFFER1\nDense #2           BUFFER1 \u2192 BUFFER3\nDense #3           BUFFER3 \u2192 BUFFER1\n</code></pre>"},{"location":"lenet-5-esp32/#visual-code-with-platformio","title":"Visual Code with PlatformIO","text":"<p>We use Visual Studio Code with PlatformIO instead of the Arduino IDE primarily because it provides better support for file-system images on ESP32 devices (when compared to Arduino IDE). All files intended for the ESP32 filesystem are placed in the project\u2019s <code>data/</code> directory.</p> <p></p> <p>The workflows are as follows.</p> <ul> <li>Build a file-system image from the contents of <code>data/</code> (all files in <code>data</code> directory to one image file:  <code>fatfs.bin</code>).</li> <li>Upload image file to the file-system partition in flash.</li> <li>Leave the application firmware unchanged.</li> </ul>"},{"location":"usps-fcn-uno/","title":"Two layer fully connected network on ATmega328 (Arduino Uno R3)","text":""},{"location":"usps-fcn-uno/#preface","title":"Preface","text":"<ul> <li>Author: <ul> <li>Auralius Manurung (auralius.manurung@ieee.org)  </li> </ul> </li> <li>Repositories: <ul> <li>Download the whole project files here (Visual Code and PlatformIO).</li> <li>Google Colab notebook </li> <li>Dataset link</li> </ul> </li> <li>Video demonstration : (speed x 2) </li> </ul>"},{"location":"usps-fcn-uno/#plan","title":"Plan","text":"<p>This project demonstrates how an SD card can be used to implement a two-layer Fully Connected Network (FCN) for digit recognition on an Arduino Uno R3.  </p> <p>The Arduino Uno R3 has extremely limited RAM and flash memory. In practical applications, especially those involving a graphical user interface, the interface alone can consume a large portion of the available resources. This makes it impractical to store intermediate activations, layer outputs, or large parameter arrays directly in memory.  </p> <p>To address this limitation, our plan is to rely on external storage in the form of an SD card. We will use the SD card to stream data that would otherwise exceed the memory capacity of the microcontroller, enabling the execution of a simple neural network pipeline under tight embedded constraints at the cost of the inference time.</p>"},{"location":"usps-fcn-uno/#hardware","title":"Hardware","text":"<ul> <li>Arduino Uno R3</li> <li>2.4 in TFT touch display (UNO shield) from MCUFRIEND</li> <li>SD Card</li> </ul>"},{"location":"usps-fcn-uno/#training","title":"Training","text":"<p>The network is designed and trained in Google Colab using the USPS dataset. The USPS dataset is selected over MNIST to minimize memory requirements. Because inference is executed directly on the ATmega328 microcontroller, strict memory constraints are encountered as early as the preprocessing stage. This is due to the device\u2019s 2 KB RAM capacity.</p> <p></p> <p>The following Python code shows the Keras implementation. Here, we set 64 neurons (\\(n=64\\)) in the hidden layer.</p> <pre><code>model = Sequential([\n    Dense(64, input_shape=(256,), activation='relu', kernel_initializer=initializer),  \n    Dropout(0.3),  # &lt;-- training only\n    Dense(10, activation='softmax', kernel_initializer=initializer)  #\n], name='two-layer-fcn')\n</code></pre> <p>After training, we can run the exporter and will receive the following files. These file will be copied to the SD Card.</p> <pre><code>/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w01.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w01.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b01.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b01.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w02.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/w02.h\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b02.txt\n/content/drive/MyDrive/NOODLE/datasets/usps/fcn2/b02.h\n</code></pre> Layer Input Output Weight files Bias files Location Fully connected #1 256 64 <code>w01.txt</code> <code>b01.txt</code> SD Card Fully connected #2 64 10 <code>w02.txt</code> <code>b02.txt</code> SD Card"},{"location":"usps-fcn-uno/#on-the-uno-side","title":"On the Uno side","text":"<pre><code>FCNFile FCN1;\nFCN1.weight_fn = \"w01.txt\";\nFCN1.bias_fn = \"b01.txt\";\nFCN1.act = ACT_RELU;\n// 256 input neurons, 64 hidden neurons\nuint16_t V = noodle_fcn(GRID, 256, 64, OUTPUT_BUFFER1, FCN1, progress_hnd);\n\nFCNFile FCN2;\nFCN2.weight_fn = \"w02.txt\";\nFCN2.bias_fn = \"b02.txt\";\nFCN2.act = ACT_SOFTMAX;\n// 10 output neurons\nV = noodle_fcn(OUTPUT_BUFFER1, V, 10, OUTPUT_BUFFER2, FCN2, progress_hnd);\n</code></pre> <p>The inference completes in  about 10 to 11 seconds. This timing is very consistent since variability only comes from data transfer from SD Card to the micrcontroller.</p>"}]}